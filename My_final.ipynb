{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60462650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import wordcloud as WordColud \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420f6a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>Photo: Image via We Heart It http://t.co/ky8Nf...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>I never knew..I better put this out to the Uni...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey just wanted to say thanks @ puberty for le...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>I'm sure coverage like the Fox News Special “T...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>@skeyno16 at u13?! I won't believe it until I ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1      #sarcasm for #people who don't understand #diy...  figurative\n",
       "2      @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3      @wilw Why do I get the feeling you like games?...  figurative\n",
       "4      -@TeacherArthurG @rweingarten You probably jus...  figurative\n",
       "...                                                  ...         ...\n",
       "81403  Photo: Image via We Heart It http://t.co/ky8Nf...     sarcasm\n",
       "81404  I never knew..I better put this out to the Uni...     sarcasm\n",
       "81405  hey just wanted to say thanks @ puberty for le...     sarcasm\n",
       "81406  I'm sure coverage like the Fox News Special “T...     sarcasm\n",
       "81407  @skeyno16 at u13?! I won't believe it until I ...     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"tweet.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd5bea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1  #sarcasm for #people who don't understand #diy...  figurative\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3  @wilw Why do I get the feeling you like games?...  figurative\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...  figurative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80333b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81408 entries, 0 to 81407\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweets  81408 non-null  object\n",
      " 1   class   81408 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b90975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "figurative    21238\n",
       "irony         20894\n",
       "sarcasm       20681\n",
       "regular       18595\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = df['class'].value_counts()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f586e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_count.plot( kind ='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469919d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c3f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@iminworkjeremy @medsingle #dailymail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw why do i get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@teacherarthurg @rweingarten you probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>photo: image via we heart it http://t.co/ky8nf...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>i never knew..i better put this out to the uni...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey just wanted to say thanks @ puberty for le...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>i'm sure coverage like the fox news special “t...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>@skeyno16 at u13?! i won't believe it until i ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      be aware  dirty step to get money  #staylight ...  figurative\n",
       "1      #sarcasm for #people who don't understand #diy...  figurative\n",
       "2      @iminworkjeremy @medsingle #dailymail readers ...  figurative\n",
       "3      @wilw why do i get the feeling you like games?...  figurative\n",
       "4      -@teacherarthurg @rweingarten you probably jus...  figurative\n",
       "...                                                  ...         ...\n",
       "81403  photo: image via we heart it http://t.co/ky8nf...     sarcasm\n",
       "81404  i never knew..i better put this out to the uni...     sarcasm\n",
       "81405  hey just wanted to say thanks @ puberty for le...     sarcasm\n",
       "81406  i'm sure coverage like the fox news special “t...     sarcasm\n",
       "81407  @skeyno16 at u13?! i won't believe it until i ...     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets']=df['tweets'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff3df72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    aware dirty step get money #staylight #staywhi...\n",
       "1    #sarcasm #people understand #diy #artattack ht...\n",
       "2    @iminworkjeremy @medsingle #dailymail readers ...\n",
       "3               @wilw get feeling like games? #sarcasm\n",
       "4    -@teacherarthurg @rweingarten probably missed ...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordsl=stopwords.words('english')\n",
    "\n",
    "def cleaning_stopwords(tweets):\n",
    "    return \" \".join([word for word in str(tweets).split() if word not in stopwordsl])\n",
    "df['tweets'] = df['tweets'].apply(lambda tweets: cleaning_stopwords(tweets))\n",
    "df['tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3623a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14070ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24076\\4253475646.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tweets'] = df['tweets'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n"
     ]
    }
   ],
   "source": [
    "df['tweets'] = df['tweets'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba92334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Usermname\n",
    "def remove_usernames_links(tweets):\n",
    "    tweets = re.sub('@[^\\s]+','',tweets)\n",
    "    tweets = re.sub('http[^\\s]+','',tweets)\n",
    "    return tweets\n",
    "df['tweets'] = df['tweets'].apply(remove_usernames_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba6e438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81403    photo image via heart   childhood cool funny s...\n",
       "81404    never knewi better put universe lolmaybe there...\n",
       "81405    hey wanted say thanks  puberty letting apart i...\n",
       "81406    im sure coverage like fox news special “the hi...\n",
       "81407                         u13 believe see it p sarcasm\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(tweets):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return tweets.translate(translator)\n",
    "df['tweets']= df['tweets'].apply(lambda tweets: cleaning_punctuations(tweets))\n",
    "df['tweets'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46122fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81403    photo image via heart   childhood cool funny s...\n",
       "81404    never knewi better put universe lolmaybe there...\n",
       "81405    hey wanted say thanks  puberty letting apart i...\n",
       "81406    im sure coverage like fox news special “the hi...\n",
       "81407                           u believe see it p sarcasm\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(tweets):\n",
    "    return re.sub('[0-9]+', '', tweets)\n",
    "df['tweets'] = df['tweets'].apply(lambda tweets: cleaning_numbers(tweets))\n",
    "df['tweets'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5586b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219d6761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweets       class\n",
      "0      aware dirty step get money staylight staywhite...  figurative\n",
      "1                sarcasm people understand diy artattack  figurative\n",
      "2      dailymail reader sensible always shocker sarca...  figurative\n",
      "3                          get feeling like game sarcasm  figurative\n",
      "4                         probably missed text sarcastic  figurative\n",
      "...                                                  ...         ...\n",
      "81403  photo image via heart childhood cool funny sar...     sarcasm\n",
      "81404  never knewi better put universe lolmaybe there...     sarcasm\n",
      "81405  hey wanted say thanks puberty letting apart it...     sarcasm\n",
      "81406  im sure coverage like fox news special “ hidde...     sarcasm\n",
      "81407                            u believe see p sarcasm     sarcasm\n",
      "\n",
      "[81408 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and limitization function\n",
    "def tokenize_and_limitize(tweet):\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the limited tokens as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(tokenize_and_limitize)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a59513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "df.to_csv('cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d5ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "# Create a copy of the dfFrame with the 'tweets' column split into individual words\n",
    "df_words = df['tweets'].str.split(expand=True).stack().reset_index(level=1, drop=True).rename('Word')\n",
    "\n",
    "# Merge the original dfFrame with the split words dfFrame\n",
    "df_merge = pd.merge(df, df_words, left_index=True, right_index=True)\n",
    "\n",
    "# Pivot the merged dfFrame to get the count of each word in each class\n",
    "pivot_df = df_merge.pivot_table(index='Word', columns='class', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Reset the index and rename the columns for the final result dfFrame\n",
    "result_df = pivot_df.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# Add the 'Total Count' column by summing the counts across all classes\n",
    "result_df['Total Count'] = result_df[['figurative', 'irony', 'regular', 'sarcasm']].sum(axis=1)\n",
    "\n",
    "# Add a column to indicate words present in all four classes\n",
    "result_df['Present in All classes'] = (result_df['figurative'] > 0) & (result_df['irony'] > 0) & (result_df['regular'] > 0) & (result_df['sarcasm'] > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aec2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result DataFrame to an Excel file\n",
    "result_df.to_excel('result1.xlsx', index=False)\n",
    "\n",
    "# Add a filter to the 'Present in All Classes' column\n",
    "book = load_workbook('result1.xlsx')\n",
    "writer = pd.ExcelWriter('result1.xlsx', engine='openpyxl')\n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "writer.sheets['Sheet1'].auto_filter.ref = writer.sheets['Sheet1'].dimensions\n",
    "\n",
    "# Save the changes to the Excel file\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13fc1d",
   "metadata": {},
   "source": [
    "### Top 300 words that are not in all of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb81e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sarcasm',\n",
       " 'longreads',\n",
       " 'funnytweets',\n",
       " 'cnndebate',\n",
       " 'pict',\n",
       " 'reuters',\n",
       " 'earring',\n",
       " 'offbeat',\n",
       " 'tinder',\n",
       " 'alanis',\n",
       " 'template',\n",
       " 'intelmm',\n",
       " 'osint',\n",
       " 'romance',\n",
       " 'fu…',\n",
       " 'fiorina',\n",
       " 'medicine',\n",
       " 'imwithhuck',\n",
       " 'edchat',\n",
       " 'gee',\n",
       " 'raining',\n",
       " 'boehner',\n",
       " 'hoting',\n",
       " 'soooo',\n",
       " 'longrea…',\n",
       " 'tilnow',\n",
       " 'breakingnews',\n",
       " 'cecilthelion',\n",
       " 'longr…',\n",
       " 'ironically',\n",
       " 'shocker',\n",
       " 'mystery',\n",
       " 'ebook',\n",
       " 'iartg',\n",
       " 'morissette',\n",
       " 'pharma',\n",
       " 'prescription',\n",
       " 'yummy',\n",
       " 'colbert',\n",
       " 'gifs',\n",
       " 'cameron',\n",
       " 'kindle',\n",
       " 'laborday',\n",
       " 'libspill',\n",
       " 'complains',\n",
       " 'eshumorcom',\n",
       " 'bachelorinparadise',\n",
       " 'claiming',\n",
       " 'peaceday',\n",
       " 'cbb',\n",
       " 'europa',\n",
       " 'pitching',\n",
       " 'pornban',\n",
       " 'slave',\n",
       " 'appreciated',\n",
       " 'bullpen',\n",
       " 'vascable',\n",
       " 'hnn',\n",
       " 'newsdict',\n",
       " 'sunny',\n",
       " 'vw',\n",
       " 'advert',\n",
       " 'amsterdam',\n",
       " 'facepalm',\n",
       " 'managed',\n",
       " 'spiritual',\n",
       " 'stumbleupon',\n",
       " 'affect',\n",
       " 'emission',\n",
       " 'thrilled',\n",
       " 'appleevent',\n",
       " 'feedly',\n",
       " 'rand',\n",
       " 'washingtonpost',\n",
       " 'allah',\n",
       " 'commenting',\n",
       " 'popefrancis',\n",
       " 'redsox',\n",
       " 'druggist',\n",
       " 'genocide',\n",
       " 'mourinho',\n",
       " 'sundries',\n",
       " 'volkswagen',\n",
       " 'autocorrect',\n",
       " 'buddha',\n",
       " 'istandwithahmed',\n",
       " 'lawsuit',\n",
       " 'pit',\n",
       " 'hating',\n",
       " 'meth',\n",
       " 'paradox',\n",
       " 'pharmacy',\n",
       " 'sassy',\n",
       " 'smarta',\n",
       " 'whoever',\n",
       " 'afc',\n",
       " 'ashleymadison',\n",
       " 'biased',\n",
       " 'cfc',\n",
       " 'coal',\n",
       " 'hitter',\n",
       " 'hows',\n",
       " 'liner',\n",
       " 'scissors',\n",
       " 'frozen',\n",
       " 'josh',\n",
       " 'kettle',\n",
       " 'rehab',\n",
       " 'throat',\n",
       " 'alanismorissette',\n",
       " 'apprentice',\n",
       " 'fave',\n",
       " 'humpday',\n",
       " 'parked',\n",
       " 'whining',\n",
       " 'wholesale',\n",
       " '•',\n",
       " 'announces',\n",
       " 'checked',\n",
       " 'copywriting',\n",
       " 'goodell',\n",
       " 'guessed',\n",
       " 'ir',\n",
       " 'kasich',\n",
       " 'mecca',\n",
       " 'moaning',\n",
       " 'settingsuccess',\n",
       " 'shelli',\n",
       " 'stellar',\n",
       " 'woo',\n",
       " 'bench',\n",
       " 'clickhere',\n",
       " 'convinced',\n",
       " 'dang',\n",
       " 'funnygifs',\n",
       " 'goody',\n",
       " 'inner',\n",
       " 'pixel',\n",
       " 'programme',\n",
       " 'reveals',\n",
       " 'spirituality',\n",
       " 'spoken',\n",
       " 'zazzle',\n",
       " 'barack',\n",
       " 'bookboost',\n",
       " 'criticizing',\n",
       " 'cunt',\n",
       " 'illegally',\n",
       " 'junk',\n",
       " 'lecturing',\n",
       " 'lottery',\n",
       " 'oneliner',\n",
       " 'politico',\n",
       " 'pouring',\n",
       " 'proofreading',\n",
       " 'rg',\n",
       " 'secular',\n",
       " 'tombrady',\n",
       " 'yolo',\n",
       " 'amid',\n",
       " 'dontvote',\n",
       " 'flu',\n",
       " 'gratitude',\n",
       " 'jamaica',\n",
       " 'l…',\n",
       " 'murderer',\n",
       " 'reminded',\n",
       " 'sox',\n",
       " 'wonderlust',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'favorited',\n",
       " 'hunger',\n",
       " 'kardashians',\n",
       " 'misspelled',\n",
       " 'morrisette',\n",
       " 'overdose',\n",
       " 'owned',\n",
       " 'ranked',\n",
       " 're',\n",
       " 'starving',\n",
       " 'stlcards',\n",
       " 'thunder',\n",
       " 'workshop',\n",
       " 'ahhh',\n",
       " 'chairman',\n",
       " 'conduct',\n",
       " 'emmy',\n",
       " 'everytime',\n",
       " 'fluent',\n",
       " 'freebook',\n",
       " 'harmony',\n",
       " 'hiv',\n",
       " 'hogan',\n",
       " 'hw',\n",
       " 'justkidding',\n",
       " 'lastest',\n",
       " 'marketresearch',\n",
       " 'mediaite',\n",
       " 'memoir',\n",
       " 'roster',\n",
       " 'slut',\n",
       " 'warns',\n",
       " 'yakub',\n",
       " 'arrive',\n",
       " 'athletics',\n",
       " 'a…',\n",
       " 'behave',\n",
       " 'blown',\n",
       " 'dilbert',\n",
       " 'educator',\n",
       " 'freeebook',\n",
       " 'grocery',\n",
       " 'hulk',\n",
       " 'ii',\n",
       " 'iphones',\n",
       " 'latepost',\n",
       " 'macdebate',\n",
       " 'meatban',\n",
       " 'mocking',\n",
       " 'mrx',\n",
       " 'olympics',\n",
       " 'qualify',\n",
       " 'rfunny',\n",
       " 'soundcloud',\n",
       " 'stressed',\n",
       " 'stylus',\n",
       " 'unbiased',\n",
       " 'understood',\n",
       " '✌',\n",
       " 'accent',\n",
       " 'bipolar',\n",
       " 'burned',\n",
       " 'childrens',\n",
       " 'complained',\n",
       " 'garage',\n",
       " 'helmet',\n",
       " 'ie',\n",
       " 'jaw',\n",
       " 'lsd',\n",
       " 'nats',\n",
       " 'oo',\n",
       " 'oriole',\n",
       " 'peeple',\n",
       " 'prank',\n",
       " 'relative',\n",
       " 'rodgers',\n",
       " 'serenity',\n",
       " 'smackdown',\n",
       " 'smartphone',\n",
       " 'soooooo',\n",
       " 'stoked',\n",
       " 'violation',\n",
       " '▸',\n",
       " 'achievement',\n",
       " 'alphabet',\n",
       " 'badass',\n",
       " 'bbad',\n",
       " 'communism',\n",
       " 'donut',\n",
       " 'gang',\n",
       " 'guessing',\n",
       " 'hoh',\n",
       " 'hunted',\n",
       " 'kalam',\n",
       " 'lasvegas',\n",
       " 'latenight',\n",
       " 'libya',\n",
       " 'mock',\n",
       " 'namaste',\n",
       " 'nextbanidea',\n",
       " 'nicknamed',\n",
       " 'objective',\n",
       " 'probe',\n",
       " 'published',\n",
       " 'remembering',\n",
       " 'resigns',\n",
       " 'saudiarabia',\n",
       " 'shoulda',\n",
       " 'smallbusiness',\n",
       " 'soo',\n",
       " 'subtweet',\n",
       " 'torture',\n",
       " 'viking',\n",
       " 'yemen',\n",
       " 'auburn',\n",
       " 'bitching',\n",
       " 'boehners',\n",
       " 'counterfeit',\n",
       " 'enforcement',\n",
       " 'eyeroll',\n",
       " 'gasp',\n",
       " 'hella',\n",
       " 'hometown',\n",
       " 'ironing',\n",
       " 'joined',\n",
       " 'mosque',\n",
       " 'ndtv',\n",
       " 'nuisance',\n",
       " 'organisation']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_not = result_df[result_df['Present in All classes'] == False].nlargest(300, 'Total Count')['Word'].tolist()\n",
    "top_words_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3a537c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['irony',\n",
       " 'ironic',\n",
       " 'news',\n",
       " 'im',\n",
       " 'love',\n",
       " 'like',\n",
       " 'people',\n",
       " 'get',\n",
       " 'amp',\n",
       " 'peace',\n",
       " 'day',\n",
       " 'late',\n",
       " 'drug',\n",
       " 'humor',\n",
       " 'education',\n",
       " 'one',\n",
       " 'politics',\n",
       " 'u',\n",
       " 'funny',\n",
       " 'time',\n",
       " 'good',\n",
       " 'know',\n",
       " 'lol',\n",
       " 'say',\n",
       " 'make',\n",
       " 'sarcastic',\n",
       " 'great',\n",
       " 'life',\n",
       " 'cant',\n",
       " 'today',\n",
       " 'gopdebate',\n",
       " 'right',\n",
       " 'see',\n",
       " 'work',\n",
       " 'really',\n",
       " 'new',\n",
       " 'go',\n",
       " 'would',\n",
       " 'oh',\n",
       " 'want',\n",
       " 'need',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'got',\n",
       " 'well',\n",
       " '’',\n",
       " 'way',\n",
       " 'never',\n",
       " 'guy',\n",
       " 'year',\n",
       " 'back',\n",
       " 'thats',\n",
       " 'going',\n",
       " 'much',\n",
       " 'look',\n",
       " 'night',\n",
       " 'via',\n",
       " 'thanks',\n",
       " 'man',\n",
       " 'take',\n",
       " 'woman',\n",
       " 'world',\n",
       " 'first',\n",
       " 'better',\n",
       " 'game',\n",
       " 'best',\n",
       " 'someone',\n",
       " 'last',\n",
       " 'fun',\n",
       " 'trump',\n",
       " 'sure',\n",
       " 'tweet',\n",
       " 'job',\n",
       " 'even',\n",
       " 'still',\n",
       " 'school',\n",
       " 'getting',\n",
       " 'come',\n",
       " 'let',\n",
       " 'always',\n",
       " 'show',\n",
       " 'could',\n",
       " 'twitter',\n",
       " 'yes',\n",
       " 'find',\n",
       " 'yet',\n",
       " 'week',\n",
       " 'another',\n",
       " 'yeah',\n",
       " 'mean',\n",
       " 'wait',\n",
       " 'call',\n",
       " 'ever',\n",
       " 'feel',\n",
       " 'start',\n",
       " 'gop',\n",
       " 'wow',\n",
       " 'nothing',\n",
       " 'he',\n",
       " 'said',\n",
       " 'use',\n",
       " 'many',\n",
       " 'kid',\n",
       " 'quote',\n",
       " 'thought',\n",
       " 'made',\n",
       " 'bad',\n",
       " 'everyone',\n",
       " 'design',\n",
       " 'next',\n",
       " 'friend',\n",
       " 'every',\n",
       " 'happy',\n",
       " 'hour',\n",
       " 'free',\n",
       " 'money',\n",
       " 'tell',\n",
       " 'morning',\n",
       " 'watching',\n",
       " 'keep',\n",
       " 'something',\n",
       " 'tech',\n",
       " '…',\n",
       " 'talk',\n",
       " 'big',\n",
       " 'live',\n",
       " 'girl',\n",
       " 'watch',\n",
       " 'fan',\n",
       " 'god',\n",
       " 'real',\n",
       " 'word',\n",
       " 'must',\n",
       " 'home',\n",
       " 'medium',\n",
       " 'ive',\n",
       " 'hate',\n",
       " 'team',\n",
       " 'video',\n",
       " 'help',\n",
       " 'play',\n",
       " 'actually',\n",
       " 'stop',\n",
       " 'na',\n",
       " 'internet',\n",
       " 'thank',\n",
       " 'read',\n",
       " 'black',\n",
       " 'white',\n",
       " 'post',\n",
       " 'nice',\n",
       " '😂',\n",
       " 'give',\n",
       " 'book',\n",
       " 'phone',\n",
       " 'tonight',\n",
       " 'car',\n",
       " 'joke',\n",
       " 'two',\n",
       " 'trying',\n",
       " 'rt',\n",
       " 'person',\n",
       " 'glad',\n",
       " 'name',\n",
       " 'run',\n",
       " 'country',\n",
       " 'also',\n",
       " 'put',\n",
       " 'end',\n",
       " 'coming',\n",
       " 'top',\n",
       " 'win',\n",
       " 'called',\n",
       " 'change',\n",
       " 'old',\n",
       " 'there',\n",
       " 'awesome',\n",
       " 'lot',\n",
       " 'talking',\n",
       " 'music',\n",
       " 'class',\n",
       " 'without',\n",
       " 'inspirational',\n",
       " 'party',\n",
       " 'social',\n",
       " 'done',\n",
       " '“',\n",
       " 'health',\n",
       " 'little',\n",
       " 'long',\n",
       " 'hope',\n",
       " 'please',\n",
       " 'state',\n",
       " 'guess',\n",
       " 'theyre',\n",
       " 'making',\n",
       " 'problem',\n",
       " 'business',\n",
       " 'shit',\n",
       " 'working',\n",
       " 'wrong',\n",
       " 'sign',\n",
       " 'service',\n",
       " 'gun',\n",
       " 'story',\n",
       " 'everything',\n",
       " 'hard',\n",
       " 'believe',\n",
       " 'food',\n",
       " 'song',\n",
       " 'sleep',\n",
       " '”',\n",
       " 'th',\n",
       " 'anyone',\n",
       " 'place',\n",
       " 'point',\n",
       " 'debate',\n",
       " 'may',\n",
       " 'saw',\n",
       " 'hey',\n",
       " 'using',\n",
       " 'playing',\n",
       " 'tv',\n",
       " 'war',\n",
       " 'haha',\n",
       " 'idea',\n",
       " 'maybe',\n",
       " 'else',\n",
       " 'family',\n",
       " 'looking',\n",
       " 'men',\n",
       " 'used',\n",
       " 'child',\n",
       " 'movie',\n",
       " 'tcot',\n",
       " 'r',\n",
       " 'around',\n",
       " 'writing',\n",
       " 'enough',\n",
       " 'football',\n",
       " 'india',\n",
       " 'w',\n",
       " 'support',\n",
       " 'saying',\n",
       " 'away',\n",
       " 'photo',\n",
       " 'true',\n",
       " 'american',\n",
       " 'gon',\n",
       " 'already',\n",
       " 'question',\n",
       " 'house',\n",
       " 'obama',\n",
       " 'care',\n",
       " 'p',\n",
       " 'since',\n",
       " 'america',\n",
       " 'yay',\n",
       " 'course',\n",
       " 'fact',\n",
       " 'truth',\n",
       " 'dog',\n",
       " 'high',\n",
       " 'totally',\n",
       " 'law',\n",
       " 'pay',\n",
       " 'ill',\n",
       " 'line',\n",
       " 'teacher',\n",
       " 'issue',\n",
       " 'lost',\n",
       " 'turn',\n",
       " 'sound',\n",
       " 'student',\n",
       " 'art',\n",
       " 'picture',\n",
       " 'police',\n",
       " 'republican',\n",
       " 'left',\n",
       " 'face',\n",
       " 'list',\n",
       " 'part',\n",
       " 'try',\n",
       " 'seems',\n",
       " 'anything',\n",
       " 'summer',\n",
       " 'email',\n",
       " 'tomorrow',\n",
       " 'cause',\n",
       " 'president',\n",
       " 'ad',\n",
       " 'boy',\n",
       " 'season',\n",
       " 'found',\n",
       " 'whats',\n",
       " 'reason',\n",
       " 'reading',\n",
       " 'check',\n",
       " 'hit',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_are = result_df[result_df['Present in All classes'] == True].nlargest(300, 'Total Count')['Word'].tolist()\n",
    "top_words_are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f795479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in top_words_are])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff0432df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0       aware dirty step staylight staywhite moralneeded  figurative\n",
       "1                       sarcasm understand diy artattack  figurative\n",
       "2      dailymail reader sensible shocker dailyfail in...  figurative\n",
       "3                                        feeling sarcasm  figurative\n",
       "4                                   probably missed text  figurative\n",
       "...                                                  ...         ...\n",
       "81403                 image heart childhood cool sarcasm     sarcasm\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm     sarcasm\n",
       "81405  wanted puberty letting apart itty bitty fuckin...     sarcasm\n",
       "81406  coverage fox special hidden harvest influence ...     sarcasm\n",
       "81407                                            sarcasm     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab0338",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "308d992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4202cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweets']\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c167076",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7035328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "635cd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce4c01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "classifier.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a6a3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92ceb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5455103795602506\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72293031",
   "metadata": {},
   "source": [
    "## COUNT vECTORIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6a3f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming you have the preprocessed tweet data and corresponding class labels stored in lists or arrays\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training data\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55e5b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the classifier (e.g., SVM)\n",
    "classifier = SVC()\n",
    "\n",
    "# Train the classifier on the count-based features and class labels\n",
    "classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "051ed7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5668836752241739\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  figurative       0.03      0.01      0.01      4179\n",
      "       irony       0.48      0.77      0.59      4276\n",
      "     regular       0.81      0.64      0.72      3696\n",
      "     sarcasm       0.64      0.86      0.73      4131\n",
      "\n",
      "    accuracy                           0.57     16282\n",
      "   macro avg       0.49      0.57      0.51     16282\n",
      "weighted avg       0.48      0.57      0.51     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have the true class labels stored in y_test\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a6ca3",
   "metadata": {},
   "source": [
    "### word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d721cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping={'figurative':0, 'irony':1, 'regular':2, 'sarcasm':3}\n",
    "df['class']=df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af336d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b287117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37236b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  class\n",
       "0       aware dirty step staylight staywhite moralneeded      0\n",
       "1                       sarcasm understand diy artattack      0\n",
       "2      dailymail reader sensible shocker dailyfail in...      0\n",
       "3                                        feeling sarcasm      0\n",
       "4                                   probably missed text      0\n",
       "...                                                  ...    ...\n",
       "81403                 image heart childhood cool sarcasm      3\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm      3\n",
       "81405  wanted puberty letting apart itty bitty fuckin...      3\n",
       "81406  coverage fox special hidden harvest influence ...      3\n",
       "81407                                            sarcasm      3\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc09786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4813290750522049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Tokenized tweets\n",
    "tokenized_tweets = [tweet.split() for tweet in df['tweets']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert tweets to average word embeddings\n",
    "tweet_embeddings = []\n",
    "for tweet in tokenized_tweets:\n",
    "    embeddings = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if embeddings:\n",
    "        tweet_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        tweet_embedding = np.zeros(100)  # Use zero vector if no word embeddings found\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Prepare data for training\n",
    "X = np.vstack(tweet_embeddings)\n",
    "y = df['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a77bf",
   "metadata": {},
   "source": [
    "## N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5717a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  class\n",
       "0       aware dirty step staylight staywhite moralneeded      0\n",
       "1                       sarcasm understand diy artattack      0\n",
       "2      dailymail reader sensible shocker dailyfail in...      0\n",
       "3                                        feeling sarcasm      0\n",
       "4                                   probably missed text      0\n",
       "...                                                  ...    ...\n",
       "81403                 image heart childhood cool sarcasm      3\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm      3\n",
       "81405  wanted puberty letting apart itty bitty fuckin...      3\n",
       "81406  coverage fox special hidden harvest influence ...      3\n",
       "81407                                            sarcasm      3\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "267b1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping={0:'figurative', 1:'irony', 2:'regular', 3:'sarcasm'}\n",
    "df['class']=df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1dbf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b218df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Count  Total Count\n",
      "(funny, sarcasm)     192       166797\n",
      "(right, sarcasm)     118       166797\n",
      "(gon, na)            107       166797\n",
      "(day, sarcasm)       104       166797\n",
      "(look, like)          98       166797\n",
      "...                  ...          ...\n",
      "(visited, mosque)      1       166797\n",
      "(never, visited)       1       166797\n",
      "(image, though)        1       166797\n",
      "(typo, image)          1       166797\n",
      "(rwc, 🏉)               1       166797\n",
      "\n",
      "[136432 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Select a particular class\n",
    "target_class = 'figurative'\n",
    "\n",
    "# Filter the dataset for the target class\n",
    "target_data = data[data['class'] == target_class]\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokenized_tweets = []\n",
    "for tweet in target_data['tweets']:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweets.append(tweet.split())\n",
    "\n",
    "# Generate bigrams\n",
    "tweet_bigrams = [list(bigrams(tweet_tokens)) for tweet_tokens in tokenized_tweets]\n",
    "\n",
    "# Flatten the list of bigrams\n",
    "all_bigrams = [bigram for tweet_bigram in tweet_bigrams for bigram in tweet_bigram]\n",
    "\n",
    "# Count the occurrences of each bigram\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "bigram_df = pd.DataFrame.from_dict(bigram_counts, orient='index', columns=['Count'])\n",
    "\n",
    "# Sort the DataFrame by the count column in descending order\n",
    "bigram_df = bigram_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Calculate the total count of bigrams in the target class\n",
    "total_count = bigram_df['Count'].sum()\n",
    "\n",
    "# Add a column with the total count for all classes\n",
    "bigram_df['Total Count'] = total_count\n",
    "\n",
    "print(bigram_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b952bdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(funny, sarcasm)</th>\n",
       "      <td>192</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(right, sarcasm)</th>\n",
       "      <td>118</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(gon, na)</th>\n",
       "      <td>107</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(day, sarcasm)</th>\n",
       "      <td>104</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(look, like)</th>\n",
       "      <td>98</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(visited, mosque)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(never, visited)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(image, though)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(typo, image)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(rwc, 🏉)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136432 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Count  Total Count\n",
       "(funny, sarcasm)     192       166797\n",
       "(right, sarcasm)     118       166797\n",
       "(gon, na)            107       166797\n",
       "(day, sarcasm)       104       166797\n",
       "(look, like)          98       166797\n",
       "...                  ...          ...\n",
       "(visited, mosque)      1       166797\n",
       "(never, visited)       1       166797\n",
       "(image, though)        1       166797\n",
       "(typo, image)          1       166797\n",
       "(rwc, 🏉)               1       166797\n",
       "\n",
       "[136432 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f3c8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "bigram_df.to_excel('result2.xlsx', index_label='Bigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31cd00f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('funny', 'sarcasm')</td>\n",
       "      <td>192</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('right', 'sarcasm')</td>\n",
       "      <td>118</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('gon', 'na')</td>\n",
       "      <td>107</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('day', 'sarcasm')</td>\n",
       "      <td>104</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('look', 'like')</td>\n",
       "      <td>98</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136427</th>\n",
       "      <td>('visited', 'mosque')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136428</th>\n",
       "      <td>('never', 'visited')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136429</th>\n",
       "      <td>('image', 'though')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136430</th>\n",
       "      <td>('typo', 'image')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136431</th>\n",
       "      <td>('rwc', '🏉')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136432 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Bigram  Count  Total Count\n",
       "0        ('funny', 'sarcasm')    192       166797\n",
       "1        ('right', 'sarcasm')    118       166797\n",
       "2               ('gon', 'na')    107       166797\n",
       "3          ('day', 'sarcasm')    104       166797\n",
       "4            ('look', 'like')     98       166797\n",
       "...                       ...    ...          ...\n",
       "136427  ('visited', 'mosque')      1       166797\n",
       "136428   ('never', 'visited')      1       166797\n",
       "136429    ('image', 'though')      1       166797\n",
       "136430      ('typo', 'image')      1       166797\n",
       "136431           ('rwc', '🏉')      1       166797\n",
       "\n",
       "[136432 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re2=pd.read_excel('result2.xlsx')\n",
    "re2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "527ff498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('funny', 'sarcasm')\",\n",
       " \"('right', 'sarcasm')\",\n",
       " \"('gon', 'na')\",\n",
       " \"('day', 'sarcasm')\",\n",
       " \"('look', 'like')\",\n",
       " \"('cant', 'wait')\",\n",
       " \"('im', 'sure')\",\n",
       " \"('fun', 'sarcasm')\",\n",
       " \"('social', 'medium')\",\n",
       " \"('last', 'night')\",\n",
       " \"('oh', 'irony')\",\n",
       " \"('great', 'sarcasm')\",\n",
       " \"('got', 'ta')\",\n",
       " \"('lol', 'sarcasm')\",\n",
       " \"('lol', 'irony')\",\n",
       " \"('time', 'sarcasm')\",\n",
       " \"('game', 'sarcasm')\",\n",
       " \"('work', 'sarcasm')\",\n",
       " \"('im', 'glad')\",\n",
       " \"('good', 'job')\",\n",
       " \"('sound', 'like')\",\n",
       " \"('better', 'sarcasm')\",\n",
       " \"('good', 'thing')\",\n",
       " \"('today', 'sarcasm')\",\n",
       " \"('job', 'sarcasm')\",\n",
       " \"('day', 'irony')\",\n",
       " \"('awesome', 'sarcasm')\",\n",
       " \"('yay', 'sarcasm')\",\n",
       " \"('love', 'sarcasm')\",\n",
       " \"('wan', 'na')\",\n",
       " \"('know', 'sarcasm')\",\n",
       " \"('lol', 'ironic')\",\n",
       " \"('year', 'sarcasm')\",\n",
       " \"('good', 'sarcasm')\",\n",
       " \"('well', 'done')\",\n",
       " \"('well', 'sarcasm')\",\n",
       " \"('feel', 'like')\",\n",
       " \"('irony', 'life')\",\n",
       " \"('year', 'ago')\",\n",
       " \"('today', 'irony')\",\n",
       " \"('love', 'people')\",\n",
       " \"('😂', 'irony')\",\n",
       " \"('life', 'sarcasm')\",\n",
       " \"('seems', 'like')\",\n",
       " \"('much', 'fun')\",\n",
       " \"('humor', 'sarcasm')\",\n",
       " \"('thank', 'god')\",\n",
       " \"('guy', 'sarcasm')\",\n",
       " \"('right', 'irony')\",\n",
       " \"('see', 'irony')\",\n",
       " \"('people', 'sarcasm')\",\n",
       " \"('tonight', 'sarcasm')\",\n",
       " \"('much', 'better')\",\n",
       " \"('ever', 'sarcasm')\",\n",
       " \"('anyone', 'else')\",\n",
       " \"('one', 'sarcasm')\",\n",
       " \"('make', 'sense')\",\n",
       " \"('life', 'irony')\",\n",
       " \"('ta', 'love')\",\n",
       " \"('last', 'year')\",\n",
       " \"('like', 'sarcasm')\",\n",
       " \"('cant', 'believe')\",\n",
       " \"('donald', 'trump')\",\n",
       " \"('sarcasm', 'lol')\",\n",
       " \"('cant', 'even')\",\n",
       " \"('ha', 'ha')\",\n",
       " \"('thing', 'sarcasm')\",\n",
       " \"('many', 'people')\",\n",
       " \"('climate', 'change')\",\n",
       " \"('kim', 'davis')\",\n",
       " \"('im', 'going')\",\n",
       " \"('one', 'day')\",\n",
       " \"('like', 'irony')\",\n",
       " \"('day', 'ironic')\",\n",
       " \"('great', 'day')\",\n",
       " \"('thanks', 'sarcasm')\",\n",
       " \"('tweet', 'sarcasm')\",\n",
       " \"('find', 'ironic')\",\n",
       " \"('shocked', 'sarcasm')\",\n",
       " \"('coming', 'sarcasm')\",\n",
       " \"('labor', 'day')\",\n",
       " \"('cant', 'get')\",\n",
       " \"('glad', 'see')\",\n",
       " \"('one', 'irony')\",\n",
       " \"('😒', 'sarcasm')\",\n",
       " \"('sarcasm', 'bb')\",\n",
       " \"('idea', 'sarcasm')\",\n",
       " \"('time', 'irony')\",\n",
       " \"('u', 'sarcasm')\",\n",
       " \"('life', 'ironic')\",\n",
       " \"('well', 'thats')\",\n",
       " \"('night', 'sarcasm')\",\n",
       " \"('looking', 'forward')\",\n",
       " \"('great', 'job')\",\n",
       " \"('im', 'shocked')\",\n",
       " \"('first', 'time')\",\n",
       " \"('fair', 'play')\",\n",
       " \"('😂', 'ironic')\",\n",
       " \"('sarcasm', 'gopdebate')\",\n",
       " \"('great', 'way')\",\n",
       " \"('twitter', 'irony')\",\n",
       " \"('people', 'like')\",\n",
       " \"('year', 'old')\",\n",
       " \"('irony', 'gopdebate')\",\n",
       " \"('wrong', 'sarcasm')\",\n",
       " \"('sense', 'sarcasm')\",\n",
       " \"('love', 'irony')\",\n",
       " \"('would', 'never')\",\n",
       " \"('problem', 'sarcasm')\",\n",
       " \"('independence', 'day')\",\n",
       " \"('get', 'better')\",\n",
       " \"('make', 'feel')\",\n",
       " \"('someone', 'else')\",\n",
       " \"('good', 'see')\",\n",
       " \"('really', 'good')\",\n",
       " \"('tweet', 'irony')\",\n",
       " \"('back', 'sarcasm')\",\n",
       " \"('😂', 'sarcasm')\",\n",
       " \"('get', 'sarcasm')\",\n",
       " \"('let', 'go')\",\n",
       " \"('best', 'sarcasm')\",\n",
       " \"('u', 'irony')\",\n",
       " \"('irony', 'lol')\",\n",
       " \"('sarcasm', 'humor')\",\n",
       " \"('pretty', 'sure')\",\n",
       " \"('via', 'irony')\",\n",
       " \"('nothing', 'like')\",\n",
       " \"('show', 'irony')\",\n",
       " \"('im', 'gon')\",\n",
       " \"('go', 'back')\",\n",
       " \"('week', 'sarcasm')\",\n",
       " \"('thing', 'irony')\",\n",
       " \"('customer', 'service')\",\n",
       " \"('nice', 'see')\",\n",
       " \"('every', 'time')\",\n",
       " \"('need', 'sarcasm')\",\n",
       " \"('say', 'sarcasm')\",\n",
       " \"('high', 'school')\",\n",
       " \"('great', 'start')\",\n",
       " \"('phone', 'irony')\",\n",
       " \"('go', 'sarcasm')\",\n",
       " \"('everyone', 'else')\",\n",
       " \"('cant', 'sleep')\",\n",
       " \"('im', 'still')\",\n",
       " \"('today', 'ironic')\",\n",
       " \"('yet', 'sarcasm')\",\n",
       " \"('done', 'sarcasm')\",\n",
       " \"('way', 'sarcasm')\",\n",
       " \"('human', 'right')\",\n",
       " \"('wait', 'see')\",\n",
       " \"('surprise', 'sarcasm')\",\n",
       " \"('sarcasm', 'sarcastic')\",\n",
       " \"('much', 'sarcasm')\",\n",
       " \"('first', 'day')\",\n",
       " \"('sure', 'sarcasm')\",\n",
       " \"('way', 'start')\",\n",
       " \"('know', 'irony')\",\n",
       " \"('oh', 'wait')\",\n",
       " \"('good', 'idea')\",\n",
       " \"('think', 'im')\",\n",
       " \"('morning', 'sarcasm')\",\n",
       " \"('next', 'week')\",\n",
       " \"('show', 'sarcasm')\",\n",
       " \"('oh', 'good')\",\n",
       " \"('job', 'irony')\",\n",
       " \"('work', 'irony')\",\n",
       " \"('team', 'sarcasm')\",\n",
       " \"('u', 'r')\",\n",
       " \"('world', 'sarcasm')\",\n",
       " \"('way', 'go')\",\n",
       " \"('im', 'watching')\",\n",
       " \"('sarcasm', 'funny')\",\n",
       " \"('everything', 'sarcasm')\",\n",
       " \"('ben', 'carson')\",\n",
       " \"('people', 'irony')\",\n",
       " \"('planned', 'parenthood')\",\n",
       " \"('gun', 'sarcasm')\",\n",
       " \"('good', 'time')\",\n",
       " \"('gun', 'control')\",\n",
       " \"('”', 'irony')\",\n",
       " \"('irony', 'ironic')\",\n",
       " \"('irony', '😂')\",\n",
       " \"('anyway', 'sarcasm')\",\n",
       " \"('would', 'love')\",\n",
       " \"('really', 'love')\",\n",
       " \"('wait', 'sarcasm')\",\n",
       " \"('get', 'irony')\",\n",
       " \"('irony', 'lost')\",\n",
       " \"('oh', 'yeah')\",\n",
       " \"('pretty', 'much')\",\n",
       " \"('west', 'ham')\",\n",
       " \"('week', 'irony')\",\n",
       " \"('back', 'work')\",\n",
       " \"('say', 'irony')\",\n",
       " \"('trying', 'get')\",\n",
       " \"('definition', 'irony')\",\n",
       " \"('oh', 'look')\",\n",
       " \"('really', 'sarcasm')\",\n",
       " \"('would', 'like')\",\n",
       " \"('ive', 'ever')\",\n",
       " \"('sarcasm', 'sarcasm')\",\n",
       " \"('think', 'sarcasm')\",\n",
       " \"('start', 'day')\",\n",
       " \"('sarcasm', 'good')\",\n",
       " \"('fan', 'sarcasm')\",\n",
       " \"('year', 'irony')\",\n",
       " \"('funny', 'irony')\",\n",
       " \"('nothing', 'better')\",\n",
       " \"('service', 'sarcasm')\",\n",
       " \"('believe', 'sarcasm')\",\n",
       " \"('really', 'want')\",\n",
       " \"('medium', 'irony')\",\n",
       " \"('sarcasm', '😂')\",\n",
       " \"('im', 'really')\",\n",
       " \"('every', 'day')\",\n",
       " \"('good', 'morning')\",\n",
       " \"('last', 'week')\",\n",
       " \"('even', 'though')\",\n",
       " \"('people', 'get')\",\n",
       " \"('getting', 'better')\",\n",
       " \"('read', 'irony')\",\n",
       " \"('want', 'see')\",\n",
       " \"('good', 'news')\",\n",
       " \"('oh', 'well')\",\n",
       " \"('back', 'irony')\",\n",
       " \"('haha', 'sarcasm')\",\n",
       " \"('like', 'im')\",\n",
       " \"('people', 'think')\",\n",
       " \"('world', 'irony')\",\n",
       " \"('know', 'im')\",\n",
       " \"('dontvote', 'sarcasm')\",\n",
       " \"('better', 'irony')\",\n",
       " \"('tv', 'show')\",\n",
       " \"('last', 'tweet')\",\n",
       " \"('make', 'sure')\",\n",
       " \"('win', 'sarcasm')\",\n",
       " \"('confederate', 'flag')\",\n",
       " \"('keep', 'getting')\",\n",
       " \"('woman', 'irony')\",\n",
       " \"('super', 'bowl')\",\n",
       " \"('work', 'today')\",\n",
       " \"('would', 'say')\",\n",
       " \"('alanis', 'morissette')\",\n",
       " \"('always', 'fun')\",\n",
       " \"('news', 'sarcasm')\",\n",
       " \"('change', 'irony')\",\n",
       " \"('smh', 'sarcasm')\",\n",
       " \"('good', 'day')\",\n",
       " \"('sarcasm', 'hashtag')\",\n",
       " \"('sarcasm', 'tag')\",\n",
       " \"('season', 'sarcasm')\",\n",
       " \"('sarcasm', 'cdnpoli')\",\n",
       " \"('see', 'coming')\",\n",
       " \"('night', 'irony')\",\n",
       " \"('tomorrow', 'sarcasm')\",\n",
       " \"('pretty', 'good')\",\n",
       " \"('shocking', 'sarcasm')\",\n",
       " \"('knew', 'sarcasm')\",\n",
       " \"('india', 'irony')\",\n",
       " \"('though', 'sarcasm')\",\n",
       " \"('im', 'excited')\",\n",
       " \"('white', 'people')\",\n",
       " \"('football', 'sarcasm')\",\n",
       " \"('im', 'sorry')\",\n",
       " \"('issue', 'irony')\",\n",
       " \"('never', 'get')\",\n",
       " \"('friend', 'sarcasm')\",\n",
       " \"('irony', 'auspol')\",\n",
       " \"('go', 'wrong')\",\n",
       " \"('ashley', 'madison')\",\n",
       " \"('weekend', 'sarcasm')\",\n",
       " \"('tcot', 'p')\",\n",
       " \"('bad', 'sarcasm')\",\n",
       " \"('one', 'ironic')\",\n",
       " \"('irony', 'irony')\",\n",
       " \"('else', 'sarcasm')\",\n",
       " \"('see', 'sarcasm')\",\n",
       " \"('put', 'sarcasm')\",\n",
       " \"('forgot', 'sarcasm')\",\n",
       " \"('game', 'irony')\",\n",
       " \"('people', 'say')\",\n",
       " \"('first', 'thing')\",\n",
       " \"('say', 'guy')\",\n",
       " \"('class', 'sarcasm')\",\n",
       " \"('want', 'sarcasm')\",\n",
       " \"('sarcasm', 'joke')\",\n",
       " \"('much', 'irony')\",\n",
       " \"('going', 'great')\",\n",
       " \"('football', 'irony')\",\n",
       " \"('love', 'getting')\",\n",
       " \"('even', 'know')\",\n",
       " \"('get', 'back')\",\n",
       " \"('😑', 'sarcasm')\",\n",
       " \"('use', 'sarcasm')\",\n",
       " \"('everyone', 'know')\",\n",
       " \"('saw', 'coming')\",\n",
       " \"('u', 'know')\",\n",
       " \"('love', 'u')\",\n",
       " \"('home', 'irony')\",\n",
       " \"('man', 'sarcasm')\",\n",
       " \"('understand', 'sarcasm')\",\n",
       " \"('start', 'sarcasm')\",\n",
       " \"('friend', 'irony')\",\n",
       " \"('sarcasm', 'quote')\",\n",
       " \"('one', 'thing')\",\n",
       " \"('well', 'irony')\",\n",
       " \"('bumper', 'sticker')\",\n",
       " \"('long', 'time')\",\n",
       " \"('look', 'good')\",\n",
       " \"('people', 'know')\",\n",
       " \"('something', 'sarcasm')\",\n",
       " \"('bb', 'sarcasm')\",\n",
       " \"('could', 'go')\",\n",
       " \"('way', 'irony')\",\n",
       " \"('he', 'going')\",\n",
       " \"('gopdebate', 'irony')\",\n",
       " \"('come', 'back')\",\n",
       " \"('much', 'love')\",\n",
       " \"('yes', 'im')\",\n",
       " \"('nice', 'sarcasm')\",\n",
       " \"('next', 'year')\",\n",
       " \"('year', 'later')\",\n",
       " \"('cool', 'sarcasm')\",\n",
       " \"('excited', 'sarcasm')\",\n",
       " \"('true', 'sarcasm')\",\n",
       " \"('like', 'ironic')\",\n",
       " \"('irony', 'rt')\",\n",
       " \"('fan', 'irony')\",\n",
       " \"('tonight', 'irony')\",\n",
       " \"('irony', 'cdnpoli')\",\n",
       " \"('ironic', 'irony')\",\n",
       " \"('lowest', 'form')\",\n",
       " \"('way', 'end')\",\n",
       " \"('friday', 'night')\",\n",
       " \"('ultimate', 'irony')\",\n",
       " \"('irony', 'india')\",\n",
       " \"('best', 'way')\",\n",
       " \"('irony', 'best')\",\n",
       " \"('twitter', 'ironic')\",\n",
       " \"('another', 'day')\",\n",
       " \"('country', 'irony')\",\n",
       " \"('best', 'friend')\",\n",
       " \"('red', 'card')\",\n",
       " \"('shit', 'sarcasm')\",\n",
       " \"('irony', 'much')\",\n",
       " \"('form', 'wit')\",\n",
       " \"('funny', 'sarcastic')\",\n",
       " \"('joy', 'sarcasm')\",\n",
       " \"('question', 'sarcasm')\",\n",
       " \"('away', 'sarcasm')\",\n",
       " \"('deal', 'sarcasm')\",\n",
       " \"('people', 'understand')\",\n",
       " \"('im', 'trying')\",\n",
       " \"('irony', 'fun')\",\n",
       " \"('power', 'irony')\",\n",
       " \"('black', 'people')\",\n",
       " \"('today', 'going')\",\n",
       " \"('day', 'work')\",\n",
       " \"('cant', 'tell')\",\n",
       " \"('yr', 'old')\",\n",
       " \"('watch', 'sarcasm')\",\n",
       " \"('word', 'sarcasm')\",\n",
       " \"('month', 'sarcasm')\",\n",
       " \"('come', 'sarcasm')\",\n",
       " \"('sleep', 'irony')\",\n",
       " \"('gopdebate', 'sarcasm')\",\n",
       " \"('twitter', 'account')\",\n",
       " \"('id', 'rather')\",\n",
       " \"('first', 'sarcasm')\",\n",
       " \"('wedding', 'day')\",\n",
       " \"('ironic', 'lol')\",\n",
       " \"('happy', 'sarcasm')\",\n",
       " \"('far', 'sarcasm')\",\n",
       " \"('im', 'getting')\",\n",
       " \"('joke', 'irony')\",\n",
       " \"('twitter', 'sarcasm')\",\n",
       " \"('going', 'get')\",\n",
       " \"('day', 'today')\",\n",
       " \"('word', 'irony')\",\n",
       " \"('anything', 'sarcasm')\",\n",
       " \"('missed', 'sarcasm')\",\n",
       " \"('know', 'whats')\",\n",
       " \"('irony', 'finest')\",\n",
       " \"('call', 'sarcasm')\",\n",
       " \"('stuff', 'sarcasm')\",\n",
       " \"('europa', 'league')\",\n",
       " \"('nothing', 'say')\",\n",
       " \"('ive', 'never')\",\n",
       " \"('sarcasm', 'rt')\",\n",
       " \"('school', 'sarcasm')\",\n",
       " \"('school', 'irony')\",\n",
       " \"('right', 'wing')\",\n",
       " \"('one', 'person')\",\n",
       " \"('know', 'would')\",\n",
       " \"('seriously', 'sarcasm')\",\n",
       " \"('cant', 'find')\",\n",
       " \"('wearing', 'shirt')\",\n",
       " \"('glad', 'got')\",\n",
       " \"('go', 'ahead')\",\n",
       " \"('want', 'go')\",\n",
       " \"('ive', 'got')\",\n",
       " \"('white', 'house')\",\n",
       " \"('funny', 'thing')\",\n",
       " \"('sarcasm', '😒')\",\n",
       " \"('hashtag', 'sarcasm')\",\n",
       " \"('add', 'sarcasm')\",\n",
       " \"('change', 'sarcasm')\",\n",
       " \"('raw', 'sarcasm')\",\n",
       " \"('oh', 'dear')\",\n",
       " \"('one', 'best')\",\n",
       " \"('god', 'irony')\",\n",
       " \"('woman', 'right')\",\n",
       " \"('mean', 'like')\",\n",
       " \"('people', 'complaining')\",\n",
       " \"('cant', 'make')\",\n",
       " \"('face', 'irony')\",\n",
       " \"('point', 'irony')\",\n",
       " \"('get', 'enough')\",\n",
       " \"('lot', 'sarcasm')\",\n",
       " \"('im', 'sarcastic')\",\n",
       " \"('make', 'want')\",\n",
       " \"('get', 'rid')\",\n",
       " \"('want', 'irony')\",\n",
       " \"('irony', 'tcot')\",\n",
       " \"('prime', 'minister')\",\n",
       " \"('others', 'irony')\",\n",
       " \"('go', 'home')\",\n",
       " \"('cell', 'phone')\",\n",
       " \"('dog', 'day')\",\n",
       " \"('parking', 'lot')\",\n",
       " \"('working', 'sarcasm')\",\n",
       " \"('free', 'speech')\",\n",
       " \"('cant', 'handle')\",\n",
       " \"('last', 'day')\",\n",
       " \"('exciting', 'sarcasm')\",\n",
       " \"('money', 'irony')\",\n",
       " \"('never', 'happens')\",\n",
       " \"('work', 'day')\",\n",
       " \"('money', 'sarcasm')\",\n",
       " \"('fault', 'sarcasm')\",\n",
       " \"('ever', 'seen')\",\n",
       " \"('wow', 'sarcasm')\",\n",
       " \"('time', 'get')\",\n",
       " \"('right', 'ironic')\",\n",
       " \"('make', 'people')\",\n",
       " \"('😂😂', 'irony')\",\n",
       " \"('sarcasm', 'raw')\",\n",
       " \"('na', 'get')\",\n",
       " \"('life', 'funny')\",\n",
       " \"('thought', 'sarcasm')\",\n",
       " \"('funny', 'way')\",\n",
       " \"('safe', 'sarcasm')\",\n",
       " \"('people', 'want')\",\n",
       " \"('irony', 'funny')\",\n",
       " \"('irony', 'hypocrisy')\",\n",
       " \"('place', 'irony')\",\n",
       " \"('guess', 'sarcasm')\",\n",
       " \"('talk', 'irony')\",\n",
       " \"('love', 'love')\",\n",
       " \"('best', 'part')\",\n",
       " \"('funny', 'humor')\",\n",
       " \"('one', 'time')\",\n",
       " \"('yeah', 'thats')\",\n",
       " \"('happen', 'sarcasm')\",\n",
       " \"('thanks', 'lot')\",\n",
       " \"('good', 'luck')\",\n",
       " \"('know', 'much')\",\n",
       " \"('see', 'people')\",\n",
       " \"('yeah', 'im')\",\n",
       " \"('get', 'see')\",\n",
       " \"('quote', 'sarcasm')\",\n",
       " \"('almost', 'got')\",\n",
       " \"('favorite', 'sarcasm')\",\n",
       " \"('people', 'still')\",\n",
       " \"('u', 'want')\",\n",
       " \"('brilliant', 'sarcasm')\",\n",
       " \"('else', 'find')\",\n",
       " \"('cant', 'afford')\",\n",
       " \"('ago', 'irony')\",\n",
       " \"('wrong', 'irony')\",\n",
       " \"('course', 'sarcasm')\",\n",
       " \"('done', 'irony')\",\n",
       " \"('amp', 'sarcasm')\",\n",
       " \"('never', 'heard')\",\n",
       " \"('politics', 'irony')\",\n",
       " \"('irony', 'elxn')\",\n",
       " \"('irony', '…')\",\n",
       " \"('wish', 'could')\",\n",
       " \"('america', 'sarcasm')\",\n",
       " \"('thats', 'great')\",\n",
       " \"('would', 'sarcasm')\",\n",
       " \"('favorite', 'thing')\",\n",
       " \"('party', 'irony')\",\n",
       " \"('see', 'one')\",\n",
       " \"('bill', 'sarcasm')\",\n",
       " \"('feel', 'good')\",\n",
       " \"('irony', 'cnndebate')\",\n",
       " \"('sarcasm', 'im')\",\n",
       " \"('th', 'independence')\",\n",
       " \"('think', 'irony')\",\n",
       " \"('smoking', 'irony')\",\n",
       " \"('well', 'im')\",\n",
       " \"('ive', 'seen')\",\n",
       " \"('irony', 'via')\",\n",
       " \"('love', 'waking')\",\n",
       " \"('help', 'sarcasm')\",\n",
       " \"('people', 'love')\",\n",
       " \"('shirt', 'irony')\",\n",
       " \"('go', 'irony')\",\n",
       " \"('day', 'get')\",\n",
       " \"('nosmoking', 'smoking')\",\n",
       " \"('make', 'u')\",\n",
       " \"('landlord', 'ultimate')\",\n",
       " \"('fun', 'landlord')\",\n",
       " \"('license', 'plate')\",\n",
       " \"('landlord', 'nosmoking')\",\n",
       " \"('come', 'irony')\",\n",
       " \"('could', 'say')\",\n",
       " \"('better', 'better')\",\n",
       " \"('would', 'make')\",\n",
       " \"('na', 'go')\",\n",
       " \"('ha', 'irony')\",\n",
       " \"('there', 'nothing')\",\n",
       " \"('sarcasm', 'p')\",\n",
       " \"('u', 'see')\",\n",
       " \"('one', 'want')\",\n",
       " \"('people', 'would')\",\n",
       " \"('hillary', 'clinton')\",\n",
       " \"('creative', 'sarcasm')\",\n",
       " \"('woman', 'sarcasm')\",\n",
       " \"('thats', 'irony')\",\n",
       " \"('president', 'irony')\",\n",
       " \"('gop', 'debate')\",\n",
       " \"('sarcastic', 'sarcasm')\",\n",
       " \"('truth', 'sarcasm')\",\n",
       " \"('never', 'sarcasm')\",\n",
       " \"('fox', 'news')\",\n",
       " \"('oh', 'im')\",\n",
       " \"('week', 'ago')\",\n",
       " \"('hard', 'work')\",\n",
       " \"('irony', 'karma')\",\n",
       " \"('tho', 'sarcasm')\",\n",
       " \"('kid', 'irony')\",\n",
       " \"('human', 'sarcasm')\",\n",
       " \"('last', 'name')\",\n",
       " \"('taylor', 'swift')\",\n",
       " \"('coach', 'sarcasm')\",\n",
       " \"('breaking', 'news')\",\n",
       " \"('every', 'single')\",\n",
       " \"('head', 'sarcasm')\",\n",
       " \"('oh', 'love')\",\n",
       " \"('home', 'sarcasm')\",\n",
       " \"('would', 'get')\",\n",
       " \"('sarcasm', 'love')\",\n",
       " \"('genius', 'sarcasm')\",\n",
       " \"('debate', 'irony')\",\n",
       " \"('really', 'irony')\",\n",
       " \"('president', 'sarcasm')\",\n",
       " \"('acting', 'like')\",\n",
       " \"('people', 'believe')\",\n",
       " \"('lovely', 'sarcasm')\",\n",
       " \"('need', 'get')\",\n",
       " \"('great', 'idea')\",\n",
       " \"('😂😂😂', 'irony')\",\n",
       " \"('country', 'sarcasm')\",\n",
       " \"('like', 'great')\",\n",
       " \"('sarcasm', 'tcot')\",\n",
       " \"('enough', 'sarcasm')\",\n",
       " \"('irony', 'alert')\",\n",
       " \"('royal', 'sarcasm')\",\n",
       " \"('kill', 'people')\",\n",
       " \"('thats', 'good')\",\n",
       " \"('people', 'talk')\",\n",
       " \"('irony', 'one')\",\n",
       " \"('😂😂😂', 'sarcasm')\",\n",
       " \"('shit', 'irony')\",\n",
       " \"('could', 'possibly')\",\n",
       " \"('cant', 'help')\",\n",
       " \"('running', 'late')\",\n",
       " \"('beautiful', 'day')\",\n",
       " \"('buy', 'new')\",\n",
       " \"('work', 'ironic')\",\n",
       " \"('im', 'pretty')\",\n",
       " \"('white', 'guy')\",\n",
       " \"('one', 'week')\",\n",
       " \"('weight', 'loss')\",\n",
       " \"('make', 'perfect')\",\n",
       " \"('move', 'sarcasm')\",\n",
       " \"('love', 'seeing')\",\n",
       " \"('issue', 'sarcasm')\",\n",
       " \"('sarcastic', 'people')\",\n",
       " \"('like', 'one')\",\n",
       " \"('new', 'york')\",\n",
       " \"('people', 'call')\",\n",
       " \"('would', 'need')\",\n",
       " \"('trump', 'irony')\",\n",
       " \"('make', 'laugh')\",\n",
       " \"('tell', 'im')\",\n",
       " \"('helpful', 'sarcasm')\",\n",
       " \"('united', 'state')\",\n",
       " \"('perfect', 'sense')\",\n",
       " \"('irony', 'bb')\",\n",
       " \"('feel', 'better')\",\n",
       " \"('people', 'go')\",\n",
       " \"('terrible', 'sarcasm')\",\n",
       " \"('😏', 'sarcasm')\",\n",
       " \"('better', 'way')\",\n",
       " \"('get', 'mad')\",\n",
       " \"('coffee', 'shop')\",\n",
       " \"('irony', 'people')\",\n",
       " \"('something', 'irony')\",\n",
       " \"('real', 'life')\",\n",
       " \"('calling', 'someone')\",\n",
       " \"('hour', 'irony')\",\n",
       " \"('amazing', 'sarcasm')\",\n",
       " \"('irony', 'hypocrite')\",\n",
       " \"('like', 'watching')\",\n",
       " \"('work', 'hard')\",\n",
       " \"('make', 'fun')\",\n",
       " \"('could', 'get')\",\n",
       " \"('let', 'u')\",\n",
       " \"('irony', 'amp')\",\n",
       " \"('heard', 'sarcasm')\",\n",
       " \"('know', 'love')\",\n",
       " \"('wait', 'go')\",\n",
       " \"('im', 'tired')\",\n",
       " \"('go', 'work')\",\n",
       " \"('shame', 'sarcasm')\",\n",
       " \"('must', 'true')\",\n",
       " \"('sarcastic', 'tweet')\",\n",
       " \"('even', 'get')\",\n",
       " \"('lmao', 'sarcasm')\",\n",
       " \"('yet', 'another')\",\n",
       " \"('look', 'great')\",\n",
       " \"('paradox', 'irony')\",\n",
       " \"('sense', 'humor')\",\n",
       " \"('last', 'time')\",\n",
       " \"('boost', 'creativity')\",\n",
       " \"('cant', 'stop')\",\n",
       " \"('like', 'good')\",\n",
       " \"('plan', 'sarcasm')\",\n",
       " \"('sunday', 'sarcasm')\",\n",
       " \"('away', 'irony')\",\n",
       " \"('ive', 'heard')\",\n",
       " \"('around', 'irony')\",\n",
       " \"('sense', 'humour')\",\n",
       " \"('sarcasm', 'like')\",\n",
       " \"('humour', 'sarcasm')\",\n",
       " \"('well', 'least')\",\n",
       " \"('think', 'ironic')\",\n",
       " \"('company', 'irony')\",\n",
       " \"('thing', 'like')\",\n",
       " \"('get', 'hurt')\",\n",
       " \"('yet', 'im')\",\n",
       " \"('decision', 'sarcasm')\",\n",
       " \"('really', 'appreciate')\",\n",
       " \"('morning', 'irony')\",\n",
       " \"('looked', 'like')\",\n",
       " \"('michael', 'vick')\",\n",
       " \"('know', 'he')\",\n",
       " \"('sarcasm', 'seriously')\",\n",
       " \"('thank', 'sarcasm')\",\n",
       " \"('national', 'dog')\",\n",
       " \"('point', 'sarcasm')\",\n",
       " \"('😂😂', 'sarcasm')\",\n",
       " \"('racist', 'sarcasm')\",\n",
       " \"('would', 'know')\",\n",
       " \"('sarcasm', 'elxn')\",\n",
       " \"('sarcasm', '😐')\",\n",
       " \"('u', 'cant')\",\n",
       " \"('best', 'thing')\",\n",
       " \"('level', 'sarcasm')\",\n",
       " \"('example', 'irony')\",\n",
       " \"('really', 'need')\",\n",
       " \"('always', 'good')\",\n",
       " \"('😊', 'sarcasm')\",\n",
       " \"('appreciate', 'sarcasm')\",\n",
       " \"('quote', 'irony')\",\n",
       " \"('let', 'know')\",\n",
       " \"('alanis', 'morrisette')\",\n",
       " \"('trump', 'sarcasm')\",\n",
       " \"('matter', 'sarcasm')\",\n",
       " \"('yeah', 'sarcasm')\",\n",
       " \"('sarcasm', 'satire')\",\n",
       " \"('really', 'really')\",\n",
       " \"('song', 'ironic')\",\n",
       " \"('call', 'irony')\",\n",
       " \"('called', 'sarcasm')\",\n",
       " \"('fell', 'asleep')\",\n",
       " \"('people', 'actually')\",\n",
       " \"('global', 'warming')\",\n",
       " \"('thing', 'ironic')\",\n",
       " \"('scott', 'walker')\",\n",
       " \"('ya', 'think')\",\n",
       " \"('ok', 'sarcasm')\",\n",
       " \"('sarcasm', 'font')\",\n",
       " \"('game', 'ironic')\",\n",
       " \"('sarcasm', 'kinda')\",\n",
       " \"('yes', 'want')\",\n",
       " \"('stupid', 'irony')\",\n",
       " \"('say', 'something')\",\n",
       " \"('time', 'ironic')\",\n",
       " \"('really', 'well')\",\n",
       " \"('cant', 'feel')\",\n",
       " \"('labour', 'party')\",\n",
       " \"('sarcasm', 'people')\",\n",
       " \"('go', 'figure')\",\n",
       " \"('fun', 'fact')\",\n",
       " \"('finally', 'get')\",\n",
       " \"('hour', 'later')\",\n",
       " \"('around', 'sarcasm')\",\n",
       " \"('like', 'everyone')\",\n",
       " \"('road', 'irony')\",\n",
       " \"('last', 'minute')\",\n",
       " \"('player', 'sent')\",\n",
       " \"('call', 'someone')\",\n",
       " \"('class', 'irony')\",\n",
       " \"('last', 'one')\",\n",
       " \"('big', 'deal')\",\n",
       " \"('bad', 'thing')\",\n",
       " \"('bit', 'sarcasm')\",\n",
       " \"('love', 'im')\",\n",
       " \"('blue', 'jay')\",\n",
       " \"('sad', 'irony')\",\n",
       " \"('good', 'one')\",\n",
       " \"('comment', 'irony')\",\n",
       " \"('run', 'irony')\",\n",
       " \"('white', 'men')\",\n",
       " \"('im', 'like')\",\n",
       " \"('wow', 'really')\",\n",
       " \"('blessed', 'sarcasm')\",\n",
       " \"('girl', 'irony')\",\n",
       " \"('important', 'sarcasm')\",\n",
       " \"('😅', 'sarcasm')\",\n",
       " \"('cute', 'sarcasm')\",\n",
       " \"('made', 'laugh')\",\n",
       " \"('new', 'quote')\",\n",
       " \"('wonder', 'sarcasm')\",\n",
       " \"('thats', 'funny')\",\n",
       " \"('government', 'sarcasm')\",\n",
       " \"('know', 'really')\",\n",
       " \"('needed', 'sarcasm')\",\n",
       " \"('hour', 'sleep')\",\n",
       " \"('coming', 'back')\",\n",
       " \"('say', 'want')\",\n",
       " \"('saturday', 'night')\",\n",
       " \"('stop', 'talking')\",\n",
       " \"('man', 'irony')\",\n",
       " \"('middle', 'east')\",\n",
       " \"('wonderful', 'sarcasm')\",\n",
       " \"('yet', 'still')\",\n",
       " \"('cant', 'say')\",\n",
       " \"('offense', 'sarcasm')\",\n",
       " \"('boy', 'sarcasm')\",\n",
       " \"('house', 'sarcasm')\",\n",
       " \"('amp', 'get')\",\n",
       " \"('love', 'life')\",\n",
       " \"('football', 'game')\",\n",
       " \"('happened', 'sarcasm')\",\n",
       " \"('ironic', 'funny')\",\n",
       " \"('christian', 'irony')\",\n",
       " \"('still', 'waiting')\",\n",
       " \"('gay', 'marriage')\",\n",
       " \"('back', 'day')\",\n",
       " \"('win', 'irony')\",\n",
       " \"('match', 'irony')\",\n",
       " \"('humour', 'irony')\",\n",
       " \"('smart', 'sarcasm')\",\n",
       " \"('ever', 'irony')\",\n",
       " \"('already', 'sarcasm')\",\n",
       " \"('reading', 'article')\",\n",
       " \"('else', 'see')\",\n",
       " \"('getting', 'rid')\",\n",
       " \"('nothing', 'irony')\",\n",
       " \"('ok', 'irony')\",\n",
       " \"('alive', 'sarcasm')\",\n",
       " \"('duh', 'sarcasm')\",\n",
       " \"('hilarious', 'irony')\",\n",
       " \"('way', 'get')\",\n",
       " \"('used', 'sarcasm')\",\n",
       " \"('book', 'irony')\",\n",
       " \"('season', 'irony')\",\n",
       " \"('red', 'light')\",\n",
       " \"('ironic', 'think')\",\n",
       " \"('might', 'well')\",\n",
       " \"('ironic', '😂')\",\n",
       " \"('anyone', 'sarcasm')\",\n",
       " \"('thanks', 'help')\",\n",
       " \"('fall', 'asleep')\",\n",
       " \"('want', 'get')\",\n",
       " \"('people', 'cant')\",\n",
       " \"('oh', 'joy')\",\n",
       " \"('sarcasm', 'via')\",\n",
       " \"('something', 'like')\",\n",
       " \"('let', 'get')\",\n",
       " \"('true', 'irony')\",\n",
       " \"('hilarious', 'sarcasm')\",\n",
       " \"('men', 'irony')\",\n",
       " \"('bad', 'guy')\",\n",
       " \"('time', 'management')\",\n",
       " \"('😉', 'sarcasm')\",\n",
       " \"('life', 'matter')\",\n",
       " \"('added', 'sarcasm')\",\n",
       " \"('fantasy', 'football')\",\n",
       " \"('irony', 'u')\",\n",
       " \"('id', 'like')\",\n",
       " \"('profile', 'pic')\",\n",
       " \"('irony', 'many')\",\n",
       " \"('reading', 'book')\",\n",
       " \"('humor', 'irony')\",\n",
       " \"('stock', 'market')\",\n",
       " \"('ironic', '😂😂')\",\n",
       " \"('people', 'make')\",\n",
       " \"('life', 'great')\",\n",
       " \"('vote', 'sarcasm')\",\n",
       " \"('oh', 'goody')\",\n",
       " \"('hypocrisy', 'irony')\",\n",
       " \"('today', 'im')\",\n",
       " \"('love', 'watching')\",\n",
       " \"('pay', 'bill')\",\n",
       " \"('white', 'person')\",\n",
       " \"('person', 'sarcasm')\",\n",
       " \"('theyre', 'going')\",\n",
       " \"('men', 'sarcasm')\",\n",
       " \"('irony', 'vmas')\",\n",
       " \"('really', 'help')\",\n",
       " \"('play', 'irony')\",\n",
       " \"('hard', 'believe')\",\n",
       " \"('year', 'ironic')\",\n",
       " \"('happy', 'birthday')\",\n",
       " \"('day', 'weekend')\",\n",
       " \"('great', 'see')\",\n",
       " \"('super', 'excited')\",\n",
       " \"('p', 'sarcasm')\",\n",
       " \"('sarcasm', 'nfl')\",\n",
       " \"('read', 'sarcasm')\",\n",
       " \"('ago', 'today')\",\n",
       " \"('look', 'bad')\",\n",
       " \"('new', 'one')\",\n",
       " \"('funny', 'ironic')\",\n",
       " \"('monday', 'sarcasm')\",\n",
       " \"('another', 'one')\",\n",
       " \"('never', 'work')\",\n",
       " \"('dream', 'meeting')\",\n",
       " \"('via', 'sarcasm')\",\n",
       " \"('man', 'dream')\",\n",
       " \"('hahaha', 'sarcasm')\",\n",
       " \"('smh', 'irony')\",\n",
       " \"('every', 'year')\",\n",
       " \"('still', 'cant')\",\n",
       " \"('sarcasm', 'wtf')\",\n",
       " \"('awkward', 'moment')\",\n",
       " \"('someone', 'like')\",\n",
       " \"('draft', 'pick')\",\n",
       " \"('feel', 'face')\",\n",
       " \"('tell', 'u')\",\n",
       " \"('story', 'sarcasm')\",\n",
       " \"('shirt', 'say')\",\n",
       " \"('racist', 'irony')\",\n",
       " \"('many', 'time')\",\n",
       " \"('work', 'well')\",\n",
       " \"('one', 'know')\",\n",
       " \"('glad', 'im')\",\n",
       " \"('ball', 'sarcasm')\",\n",
       " \"('amp', 'im')\",\n",
       " \"('muslim', 'irony')\",\n",
       " \"('service', 'irony')\",\n",
       " \"('law', 'irony')\",\n",
       " \"('food', 'irony')\",\n",
       " \"('day', 'summer')\",\n",
       " \"('let', 'see')\",\n",
       " \"('say', 'anything')\",\n",
       " \"('bring', 'back')\",\n",
       " \"('funny', 'lol')\",\n",
       " \"('read', 'article')\",\n",
       " \"('great', 'news')\",\n",
       " \"('next', 'day')\",\n",
       " \"('little', 'bit')\",\n",
       " \"('sarcasm', 'work')\",\n",
       " \"('sarcasm', 'make')\",\n",
       " \"('hate', 'people')\",\n",
       " \"('exactly', 'like')\",\n",
       " \"('hour', 'work')\",\n",
       " \"('end', 'world')\",\n",
       " \"('notreally', 'sarcasm')\",\n",
       " \"('oh', 'yea')\",\n",
       " \"('omg', 'sarcasm')\",\n",
       " \"('play', 'league')\",\n",
       " \"('make', 'much')\",\n",
       " \"('world', 'live')\",\n",
       " \"('thats', 'right')\",\n",
       " \"('medium', 'sarcasm')\",\n",
       " \"('would', 'thought')\",\n",
       " \"('ad', 'irony')\",\n",
       " \"('church', 'irony')\",\n",
       " \"('people', 'never')\",\n",
       " \"('sleep', 'ironic')\",\n",
       " \"('someone', 'el')\",\n",
       " \"('make', 'irony')\",\n",
       " \"('right', 'next')\",\n",
       " \"('ball', 'irony')\",\n",
       " \"('auspol', 'irony')\",\n",
       " \"('thank', 'goodness')\",\n",
       " \"('note', 'self')\",\n",
       " \"('wait', 'get')\",\n",
       " \"('p', 'irony')\",\n",
       " \"('technology', 'irony')\",\n",
       " \"('thats', 'real')\",\n",
       " \"('food', 'sarcasm')\",\n",
       " \"('great', 'time')\",\n",
       " \"('end', 'irony')\",\n",
       " \"('room', 'sarcasm')\",\n",
       " \"('oh', 'thats')\",\n",
       " \"('get', 'dumped')\",\n",
       " \"('dumped', 'acting')\",\n",
       " \"('ironic', 'thing')\",\n",
       " \"('funny', 'people')\",\n",
       " \"('would', 'think')\",\n",
       " \"('dont', 'know')\",\n",
       " \"('know', 'u')\",\n",
       " \"('day', 'amp')\",\n",
       " \"('minute', 'irony')\",\n",
       " \"('good', 'irony')\",\n",
       " \"('last', 'thing')\",\n",
       " \"('black', 'woman')\",\n",
       " \"('control', 'sarcasm')\",\n",
       " \"('never', 'thought')\",\n",
       " \"('making', 'fun')\",\n",
       " \"('need', 'gun')\",\n",
       " \"('huh', 'sarcasm')\",\n",
       " \"('cant', 'go')\",\n",
       " \"('im', 'happy')\",\n",
       " \"('journalism', 'sarcasm')\",\n",
       " \"('irony', 'proofreading')\",\n",
       " \"('day', 'without')\",\n",
       " \"('live', 'sarcasm')\",\n",
       " \"('joke', 'sarcasm')\",\n",
       " \"('god', 'sarcasm')\",\n",
       " \"('player', 'sarcasm')\",\n",
       " \"('anymore', 'sarcasm')\",\n",
       " \"('sarcasm', 'smh')\",\n",
       " \"('looking', 'like')\",\n",
       " \"('ironic', 'life')\",\n",
       " \"('let', 'make')\",\n",
       " \"('oh', 'yay')\",\n",
       " \"('office', 'sarcasm')\",\n",
       " \"('thing', 'u')\",\n",
       " \"('weird', 'sarcasm')\",\n",
       " \"('new', 'orleans')\",\n",
       " \"('proud', 'sarcasm')\",\n",
       " \"('get', 'paid')\",\n",
       " \"('take', 'away')\",\n",
       " \"('winning', 'sarcasm')\",\n",
       " \"('radio', 'irony')\",\n",
       " \"('day', 'like')\",\n",
       " \"('break', 'irony')\",\n",
       " \"('irony', '😂😂😂')\",\n",
       " \"('passive', 'aggressive')\",\n",
       " \"('biggest', 'irony')\",\n",
       " \"('cant', 'stand')\",\n",
       " \"('tweet', 'ironic')\",\n",
       " \"('perfect', 'sarcasm')\",\n",
       " \"('two', 'day')\",\n",
       " \"('list', 'sarcasm')\",\n",
       " \"('video', 'sarcasm')\",\n",
       " \"('there', 'irony')\",\n",
       " \"('comment', 'sarcasm')\",\n",
       " \"('truck', 'irony')\",\n",
       " \"('anyone', 'ever')\",\n",
       " \"('wait', 'hear')\",\n",
       " \"('video', 'irony')\",\n",
       " \"('love', 'going')\",\n",
       " \"('appreciated', 'sarcasm')\",\n",
       " \"('sarcasm', 'best')\",\n",
       " \"('thing', 'get')\",\n",
       " \"('today', 'gon')\",\n",
       " \"('sore', 'throat')\",\n",
       " \"('really', 'mean')\",\n",
       " \"('paper', 'irony')\",\n",
       " \"('time', 'year')\",\n",
       " \"('people', 'really')\",\n",
       " \"('like', 'need')\",\n",
       " \"('ice', 'cream')\",\n",
       " \"('else', 'think')\",\n",
       " \"('pm', 'irony')\",\n",
       " \"('obama', 'sarcasm')\",\n",
       " \"('birthday', 'irony')\",\n",
       " \"('blog', 'post')\",\n",
       " \"('get', 'old')\",\n",
       " \"('said', 'want')\",\n",
       " \"('hope', 'amp')\",\n",
       " \"('thats', 'ironic')\",\n",
       " \"('going', 'make')\",\n",
       " \"('night', 'ironic')\",\n",
       " \"('fighting', 'irony')\",\n",
       " \"('always', 'make')\",\n",
       " \"('need', 'new')\",\n",
       " \"('one', 'see')\",\n",
       " \"('phone', 'call')\",\n",
       " \"('look', 'irony')\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bi = re2.nlargest(1000, 'Count')['Bigram'].tolist()\n",
    "top_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4185d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = ' '.join([word for word in text.split() if word not in top_bi])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae4f53",
   "metadata": {},
   "source": [
    "# N = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f52333de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Count  Total Count\n",
      "(got, ta, love)                                 29       145582\n",
      "(im, gon, na)                                   20       145582\n",
      "(much, fun, sarcasm)                            20       145582\n",
      "(cant, wait, see)                               16       145582\n",
      "(good, job, sarcasm)                            16       145582\n",
      "...                                            ...          ...\n",
      "(lifeimitatesart, irony, albertafreestore…)      1       145582\n",
      "(store, lifeimitatesart, irony)                  1       145582\n",
      "(free, store, lifeimitatesart)                   1       145582\n",
      "(slipper, free, store)                           1       145582\n",
      "(sarcasm, rwc, 🏉)                                1       145582\n",
      "\n",
      "[142805 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Select a particular class\n",
    "target_class = 'figurative'\n",
    "\n",
    "# Filter the dataset for the target class\n",
    "target_data = data[data['class'] == target_class]\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokenized_tweets = []\n",
    "for tweet in target_data['tweets']:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweets.append(tweet.split())\n",
    "\n",
    "# Generate trigrams\n",
    "tweet_trigrams = [list(trigrams(tweet_tokens)) for tweet_tokens in tokenized_tweets]\n",
    "\n",
    "# Flatten the list of trigrams\n",
    "all_trigrams = [trigram for tweet_trigram in tweet_trigrams for trigram in tweet_trigram]\n",
    "\n",
    "# Count the occurrences of each trigram\n",
    "trigram_counts = Counter(all_trigrams)\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "trigram_df = pd.DataFrame.from_dict(trigram_counts, orient='index', columns=['Count'])\n",
    "\n",
    "# Sort the DataFrame by the count column in descending order\n",
    "trigram_df = trigram_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Calculate the total count of trigrams in the target class\n",
    "total_count = trigram_df['Count'].sum()\n",
    "\n",
    "# Add a column with the total count for all classes\n",
    "trigram_df['Total Count'] = total_count\n",
    "\n",
    "print(trigram_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ec20627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(got, ta, love)</th>\n",
       "      <td>29</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(im, gon, na)</th>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(much, fun, sarcasm)</th>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cant, wait, see)</th>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(good, job, sarcasm)</th>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(lifeimitatesart, irony, albertafreestore…)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(store, lifeimitatesart, irony)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(free, store, lifeimitatesart)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(slipper, free, store)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sarcasm, rwc, 🏉)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142805 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Count  Total Count\n",
       "(got, ta, love)                                 29       145582\n",
       "(im, gon, na)                                   20       145582\n",
       "(much, fun, sarcasm)                            20       145582\n",
       "(cant, wait, see)                               16       145582\n",
       "(good, job, sarcasm)                            16       145582\n",
       "...                                            ...          ...\n",
       "(lifeimitatesart, irony, albertafreestore…)      1       145582\n",
       "(store, lifeimitatesart, irony)                  1       145582\n",
       "(free, store, lifeimitatesart)                   1       145582\n",
       "(slipper, free, store)                           1       145582\n",
       "(sarcasm, rwc, 🏉)                                1       145582\n",
       "\n",
       "[142805 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61208780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "trigram_df.to_excel('result3.xlsx', index_label='trigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80998094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('got', 'ta', 'love')</td>\n",
       "      <td>29</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('im', 'gon', 'na')</td>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('much', 'fun', 'sarcasm')</td>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('cant', 'wait', 'see')</td>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('good', 'job', 'sarcasm')</td>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142800</th>\n",
       "      <td>('lifeimitatesart', 'irony', 'albertafreestore…')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142801</th>\n",
       "      <td>('store', 'lifeimitatesart', 'irony')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142802</th>\n",
       "      <td>('free', 'store', 'lifeimitatesart')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142803</th>\n",
       "      <td>('slipper', 'free', 'store')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142804</th>\n",
       "      <td>('sarcasm', 'rwc', '🏉')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  trigram  Count  Total Count\n",
       "0                                   ('got', 'ta', 'love')     29       145582\n",
       "1                                     ('im', 'gon', 'na')     20       145582\n",
       "2                              ('much', 'fun', 'sarcasm')     20       145582\n",
       "3                                 ('cant', 'wait', 'see')     16       145582\n",
       "4                              ('good', 'job', 'sarcasm')     16       145582\n",
       "...                                                   ...    ...          ...\n",
       "142800  ('lifeimitatesart', 'irony', 'albertafreestore…')      1       145582\n",
       "142801              ('store', 'lifeimitatesart', 'irony')      1       145582\n",
       "142802               ('free', 'store', 'lifeimitatesart')      1       145582\n",
       "142803                       ('slipper', 'free', 'store')      1       145582\n",
       "142804                            ('sarcasm', 'rwc', '🏉')      1       145582\n",
       "\n",
       "[142805 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re3=pd.read_excel('result3.xlsx')\n",
    "re3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6842d022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('got', 'ta', 'love')\",\n",
       " \"('im', 'gon', 'na')\",\n",
       " \"('much', 'fun', 'sarcasm')\",\n",
       " \"('cant', 'wait', 'see')\",\n",
       " \"('good', 'job', 'sarcasm')\",\n",
       " \"('social', 'medium', 'irony')\",\n",
       " \"('keep', 'getting', 'better')\",\n",
       " \"('lowest', 'form', 'wit')\",\n",
       " \"('ha', 'ha', 'ha')\",\n",
       " \"('great', 'day', 'sarcasm')\",\n",
       " \"('see', 'coming', 'sarcasm')\",\n",
       " \"('well', 'done', 'sarcasm')\",\n",
       " \"('irony', 'fun', 'landlord')\",\n",
       " \"('ultimate', 'irony', 'fun')\",\n",
       " \"('landlord', 'ultimate', 'irony')\",\n",
       " \"('fun', 'landlord', 'nosmoking')\",\n",
       " \"('nosmoking', 'smoking', 'irony')\",\n",
       " \"('landlord', 'nosmoking', 'smoking')\",\n",
       " \"('make', 'sense', 'sarcasm')\",\n",
       " \"('always', 'fun', 'sarcasm')\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tri = re3.nlargest(20, 'Count')['trigram'].tolist()\n",
    "top_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df05ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = ' '.join([word for word in text.split() if word not in top_tri])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74a809f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible always shocker sarca...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get feeling like game sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text sarcastic</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>photo image via heart childhood cool funny sar...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>never knewi better put universe lolmaybe there...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey wanted say thanks puberty letting apart it...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>im sure coverage like fox news special “ hidde...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>u believe see p sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      aware dirty step get money staylight staywhite...  figurative\n",
       "1                sarcasm people understand diy artattack  figurative\n",
       "2      dailymail reader sensible always shocker sarca...  figurative\n",
       "3                          get feeling like game sarcasm  figurative\n",
       "4                         probably missed text sarcastic  figurative\n",
       "...                                                  ...         ...\n",
       "81403  photo image via heart childhood cool funny sar...     sarcasm\n",
       "81404  never knewi better put universe lolmaybe there...     sarcasm\n",
       "81405  hey wanted say thanks puberty letting apart it...     sarcasm\n",
       "81406  im sure coverage like fox news special “ hidde...     sarcasm\n",
       "81407                            u believe see p sarcasm     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01f4152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7240510993735413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  figurative       0.34      0.07      0.12      4179\n",
      "       irony       0.67      0.93      0.78      4276\n",
      "     regular       0.99      1.00      1.00      3696\n",
      "     sarcasm       0.66      0.93      0.77      4131\n",
      "\n",
      "    accuracy                           0.72     16282\n",
      "   macro avg       0.67      0.73      0.67     16282\n",
      "weighted avg       0.66      0.72      0.66     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenized tweets\n",
    "tokenized_tweets = [tweet.split() for tweet in df['tweets']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert tweets to average word embeddings\n",
    "tweet_embeddings = []\n",
    "for tweet in tokenized_tweets:\n",
    "    embeddings = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if embeddings:\n",
    "        tweet_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        tweet_embedding = np.zeros(100)  # Use zero vector if no word embeddings found\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Prepare data for training\n",
    "X = np.vstack(tweet_embeddings)\n",
    "y = df['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "327ccb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10434390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  figurative       0.11      0.09      0.10      4179\n",
      "       irony       0.43      0.55      0.48      4276\n",
      "     regular       0.93      0.74      0.82      3696\n",
      "     sarcasm       0.54      0.55      0.54      4131\n",
      "\n",
      "    accuracy                           0.47     16282\n",
      "   macro avg       0.50      0.48      0.49     16282\n",
      "weighted avg       0.49      0.47      0.48     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the tweet data and labels\n",
    "\n",
    "tweets = df['tweets']\n",
    "labels = df['class']\n",
    "\n",
    "# Preprocess the tweets (e.g., lowercasing, removing punctuation, etc.)\n",
    "# ...\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the vectorizer with bigram features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Transform the training data into bigram feature vectors\n",
    "train_features = vectorizer.fit_transform(train_tweets)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Build a classification model (e.g., logistic regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2247c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdf9f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(lr_model, open('clsf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39bd52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenized_tweets, open('tokenized.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbcca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
