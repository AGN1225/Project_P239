{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60462650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import wordcloud as WordColud \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420f6a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>Photo: Image via We Heart It http://t.co/ky8Nf...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>I never knew..I better put this out to the Uni...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey just wanted to say thanks @ puberty for le...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>I'm sure coverage like the Fox News Special “T...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>@skeyno16 at u13?! I won't believe it until I ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1      #sarcasm for #people who don't understand #diy...  figurative\n",
       "2      @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3      @wilw Why do I get the feeling you like games?...  figurative\n",
       "4      -@TeacherArthurG @rweingarten You probably jus...  figurative\n",
       "...                                                  ...         ...\n",
       "81403  Photo: Image via We Heart It http://t.co/ky8Nf...     sarcasm\n",
       "81404  I never knew..I better put this out to the Uni...     sarcasm\n",
       "81405  hey just wanted to say thanks @ puberty for le...     sarcasm\n",
       "81406  I'm sure coverage like the Fox News Special “T...     sarcasm\n",
       "81407  @skeyno16 at u13?! I won't believe it until I ...     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"tweet.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd5bea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1  #sarcasm for #people who don't understand #diy...  figurative\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3  @wilw Why do I get the feeling you like games?...  figurative\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...  figurative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80333b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81408 entries, 0 to 81407\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweets  81408 non-null  object\n",
      " 1   class   81408 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b90975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "figurative    21238\n",
       "irony         20894\n",
       "sarcasm       20681\n",
       "regular       18595\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = df['class'].value_counts()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f586e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_count.plot( kind ='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469919d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sarth\n",
      "[nltk_data]     Naik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sarth\n",
      "[nltk_data]     Naik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sarth\n",
      "[nltk_data]     Naik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c3f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@iminworkjeremy @medsingle #dailymail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw why do i get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@teacherarthurg @rweingarten you probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>photo: image via we heart it http://t.co/ky8nf...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>i never knew..i better put this out to the uni...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey just wanted to say thanks @ puberty for le...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>i'm sure coverage like the fox news special “t...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>@skeyno16 at u13?! i won't believe it until i ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      be aware  dirty step to get money  #staylight ...  figurative\n",
       "1      #sarcasm for #people who don't understand #diy...  figurative\n",
       "2      @iminworkjeremy @medsingle #dailymail readers ...  figurative\n",
       "3      @wilw why do i get the feeling you like games?...  figurative\n",
       "4      -@teacherarthurg @rweingarten you probably jus...  figurative\n",
       "...                                                  ...         ...\n",
       "81403  photo: image via we heart it http://t.co/ky8nf...     sarcasm\n",
       "81404  i never knew..i better put this out to the uni...     sarcasm\n",
       "81405  hey just wanted to say thanks @ puberty for le...     sarcasm\n",
       "81406  i'm sure coverage like the fox news special “t...     sarcasm\n",
       "81407  @skeyno16 at u13?! i won't believe it until i ...     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets']=df['tweets'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff3df72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    aware dirty step get money #staylight #staywhi...\n",
       "1    #sarcasm #people understand #diy #artattack ht...\n",
       "2    @iminworkjeremy @medsingle #dailymail readers ...\n",
       "3               @wilw get feeling like games? #sarcasm\n",
       "4    -@teacherarthurg @rweingarten probably missed ...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordsl=stopwords.words('english')\n",
    "\n",
    "def cleaning_stopwords(tweets):\n",
    "    return \" \".join([word for word in str(tweets).split() if word not in stopwordsl])\n",
    "df['tweets'] = df['tweets'].apply(lambda tweets: cleaning_stopwords(tweets))\n",
    "df['tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3623a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14070ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarth Naik\\AppData\\Local\\Temp\\ipykernel_32404\\4253475646.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tweets'] = df['tweets'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n"
     ]
    }
   ],
   "source": [
    "df['tweets'] = df['tweets'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba92334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Usermname\n",
    "def remove_usernames_links(tweets):\n",
    "    tweets = re.sub('@[^\\s]+','',tweets)\n",
    "    tweets = re.sub('http[^\\s]+','',tweets)\n",
    "    return tweets\n",
    "df['tweets'] = df['tweets'].apply(remove_usernames_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba6e438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81403    photo image via heart   childhood cool funny s...\n",
       "81404    never knewi better put universe lolmaybe there...\n",
       "81405    hey wanted say thanks  puberty letting apart i...\n",
       "81406    im sure coverage like fox news special “the hi...\n",
       "81407                         u13 believe see it p sarcasm\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(tweets):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return tweets.translate(translator)\n",
    "df['tweets']= df['tweets'].apply(lambda tweets: cleaning_punctuations(tweets))\n",
    "df['tweets'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46122fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81403    photo image via heart   childhood cool funny s...\n",
       "81404    never knewi better put universe lolmaybe there...\n",
       "81405    hey wanted say thanks  puberty letting apart i...\n",
       "81406    im sure coverage like fox news special “the hi...\n",
       "81407                           u believe see it p sarcasm\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(tweets):\n",
    "    return re.sub('[0-9]+', '', tweets)\n",
    "df['tweets'] = df['tweets'].apply(lambda tweets: cleaning_numbers(tweets))\n",
    "df['tweets'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5586b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sarth\n",
      "[nltk_data]     Naik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219d6761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweets       class\n",
      "0      aware dirty step get money staylight staywhite...  figurative\n",
      "1                sarcasm people understand diy artattack  figurative\n",
      "2      dailymail reader sensible always shocker sarca...  figurative\n",
      "3                          get feeling like game sarcasm  figurative\n",
      "4                         probably missed text sarcastic  figurative\n",
      "...                                                  ...         ...\n",
      "81403  photo image via heart childhood cool funny sar...     sarcasm\n",
      "81404  never knewi better put universe lolmaybe there...     sarcasm\n",
      "81405  hey wanted say thanks puberty letting apart it...     sarcasm\n",
      "81406  im sure coverage like fox news special “ hidde...     sarcasm\n",
      "81407                            u believe see p sarcasm     sarcasm\n",
      "\n",
      "[81408 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and limitization function\n",
    "def tokenize_and_limitize(tweet):\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the limited tokens as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(tokenize_and_limitize)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a59513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "df.to_csv('cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d5ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "# Create a copy of the dfFrame with the 'tweets' column split into individual words\n",
    "df_words = df['tweets'].str.split(expand=True).stack().reset_index(level=1, drop=True).rename('Word')\n",
    "\n",
    "# Merge the original dfFrame with the split words dfFrame\n",
    "df_merge = pd.merge(df, df_words, left_index=True, right_index=True)\n",
    "\n",
    "# Pivot the merged dfFrame to get the count of each word in each class\n",
    "pivot_df = df_merge.pivot_table(index='Word', columns='class', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Reset the index and rename the columns for the final result dfFrame\n",
    "result_df = pivot_df.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# Add the 'Total Count' column by summing the counts across all classes\n",
    "result_df['Total Count'] = result_df[['figurative', 'irony', 'regular', 'sarcasm']].sum(axis=1)\n",
    "\n",
    "# Add a column to indicate words present in all four classes\n",
    "result_df['Present in All classes'] = (result_df['figurative'] > 0) & (result_df['irony'] > 0) & (result_df['regular'] > 0) & (result_df['sarcasm'] > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aec2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result DataFrame to an Excel file\n",
    "result_df.to_excel('result1.xlsx', index=False)\n",
    "\n",
    "# Add a filter to the 'Present in All Classes' column\n",
    "book = load_workbook('result1.xlsx')\n",
    "writer = pd.ExcelWriter('result1.xlsx', engine='openpyxl')\n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "writer.sheets['Sheet1'].auto_filter.ref = writer.sheets['Sheet1'].dimensions\n",
    "\n",
    "# Save the changes to the Excel file\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13fc1d",
   "metadata": {},
   "source": [
    "### Top 300 words that are not in all of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb81e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sarcasm',\n",
       " 'longreads',\n",
       " 'funnytweets',\n",
       " 'cnndebate',\n",
       " 'pict',\n",
       " 'reuters',\n",
       " 'earring',\n",
       " 'offbeat',\n",
       " 'tinder',\n",
       " 'alanis',\n",
       " 'template',\n",
       " 'intelmm',\n",
       " 'osint',\n",
       " 'romance',\n",
       " 'fu…',\n",
       " 'fiorina',\n",
       " 'medicine',\n",
       " 'imwithhuck',\n",
       " 'edchat',\n",
       " 'gee',\n",
       " 'raining',\n",
       " 'boehner',\n",
       " 'hoting',\n",
       " 'soooo',\n",
       " 'longrea…',\n",
       " 'tilnow',\n",
       " 'breakingnews',\n",
       " 'cecilthelion',\n",
       " 'longr…',\n",
       " 'ironically',\n",
       " 'shocker',\n",
       " 'mystery',\n",
       " 'ebook',\n",
       " 'iartg',\n",
       " 'morissette',\n",
       " 'pharma',\n",
       " 'prescription',\n",
       " 'yummy',\n",
       " 'colbert',\n",
       " 'gifs',\n",
       " 'cameron',\n",
       " 'kindle',\n",
       " 'laborday',\n",
       " 'libspill',\n",
       " 'complains',\n",
       " 'eshumorcom',\n",
       " 'bachelorinparadise',\n",
       " 'claiming',\n",
       " 'peaceday',\n",
       " 'cbb',\n",
       " 'europa',\n",
       " 'pitching',\n",
       " 'pornban',\n",
       " 'slave',\n",
       " 'appreciated',\n",
       " 'bullpen',\n",
       " 'vascable',\n",
       " 'hnn',\n",
       " 'newsdict',\n",
       " 'sunny',\n",
       " 'vw',\n",
       " 'advert',\n",
       " 'amsterdam',\n",
       " 'facepalm',\n",
       " 'managed',\n",
       " 'spiritual',\n",
       " 'stumbleupon',\n",
       " 'affect',\n",
       " 'emission',\n",
       " 'thrilled',\n",
       " 'appleevent',\n",
       " 'feedly',\n",
       " 'rand',\n",
       " 'washingtonpost',\n",
       " 'allah',\n",
       " 'commenting',\n",
       " 'popefrancis',\n",
       " 'redsox',\n",
       " 'druggist',\n",
       " 'genocide',\n",
       " 'mourinho',\n",
       " 'sundries',\n",
       " 'volkswagen',\n",
       " 'autocorrect',\n",
       " 'buddha',\n",
       " 'istandwithahmed',\n",
       " 'lawsuit',\n",
       " 'pit',\n",
       " 'hating',\n",
       " 'meth',\n",
       " 'paradox',\n",
       " 'pharmacy',\n",
       " 'sassy',\n",
       " 'smarta',\n",
       " 'whoever',\n",
       " 'afc',\n",
       " 'ashleymadison',\n",
       " 'biased',\n",
       " 'cfc',\n",
       " 'coal',\n",
       " 'hitter',\n",
       " 'hows',\n",
       " 'liner',\n",
       " 'scissors',\n",
       " 'frozen',\n",
       " 'josh',\n",
       " 'kettle',\n",
       " 'rehab',\n",
       " 'throat',\n",
       " 'alanismorissette',\n",
       " 'apprentice',\n",
       " 'fave',\n",
       " 'humpday',\n",
       " 'parked',\n",
       " 'whining',\n",
       " 'wholesale',\n",
       " '•',\n",
       " 'announces',\n",
       " 'checked',\n",
       " 'copywriting',\n",
       " 'goodell',\n",
       " 'guessed',\n",
       " 'ir',\n",
       " 'kasich',\n",
       " 'mecca',\n",
       " 'moaning',\n",
       " 'settingsuccess',\n",
       " 'shelli',\n",
       " 'stellar',\n",
       " 'woo',\n",
       " 'bench',\n",
       " 'clickhere',\n",
       " 'convinced',\n",
       " 'dang',\n",
       " 'funnygifs',\n",
       " 'goody',\n",
       " 'inner',\n",
       " 'pixel',\n",
       " 'programme',\n",
       " 'reveals',\n",
       " 'spirituality',\n",
       " 'spoken',\n",
       " 'zazzle',\n",
       " 'barack',\n",
       " 'bookboost',\n",
       " 'criticizing',\n",
       " 'cunt',\n",
       " 'illegally',\n",
       " 'junk',\n",
       " 'lecturing',\n",
       " 'lottery',\n",
       " 'oneliner',\n",
       " 'politico',\n",
       " 'pouring',\n",
       " 'proofreading',\n",
       " 'rg',\n",
       " 'secular',\n",
       " 'tombrady',\n",
       " 'yolo',\n",
       " 'amid',\n",
       " 'dontvote',\n",
       " 'flu',\n",
       " 'gratitude',\n",
       " 'jamaica',\n",
       " 'l…',\n",
       " 'murderer',\n",
       " 'reminded',\n",
       " 'sox',\n",
       " 'wonderlust',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'favorited',\n",
       " 'hunger',\n",
       " 'kardashians',\n",
       " 'misspelled',\n",
       " 'morrisette',\n",
       " 'overdose',\n",
       " 'owned',\n",
       " 'ranked',\n",
       " 're',\n",
       " 'starving',\n",
       " 'stlcards',\n",
       " 'thunder',\n",
       " 'workshop',\n",
       " 'ahhh',\n",
       " 'chairman',\n",
       " 'conduct',\n",
       " 'emmy',\n",
       " 'everytime',\n",
       " 'fluent',\n",
       " 'freebook',\n",
       " 'harmony',\n",
       " 'hiv',\n",
       " 'hogan',\n",
       " 'hw',\n",
       " 'justkidding',\n",
       " 'lastest',\n",
       " 'marketresearch',\n",
       " 'mediaite',\n",
       " 'memoir',\n",
       " 'roster',\n",
       " 'slut',\n",
       " 'warns',\n",
       " 'yakub',\n",
       " 'arrive',\n",
       " 'athletics',\n",
       " 'a…',\n",
       " 'behave',\n",
       " 'blown',\n",
       " 'dilbert',\n",
       " 'educator',\n",
       " 'freeebook',\n",
       " 'grocery',\n",
       " 'hulk',\n",
       " 'ii',\n",
       " 'iphones',\n",
       " 'latepost',\n",
       " 'macdebate',\n",
       " 'meatban',\n",
       " 'mocking',\n",
       " 'mrx',\n",
       " 'olympics',\n",
       " 'qualify',\n",
       " 'rfunny',\n",
       " 'soundcloud',\n",
       " 'stressed',\n",
       " 'stylus',\n",
       " 'unbiased',\n",
       " 'understood',\n",
       " '✌',\n",
       " 'accent',\n",
       " 'bipolar',\n",
       " 'burned',\n",
       " 'childrens',\n",
       " 'complained',\n",
       " 'garage',\n",
       " 'helmet',\n",
       " 'ie',\n",
       " 'jaw',\n",
       " 'lsd',\n",
       " 'nats',\n",
       " 'oo',\n",
       " 'oriole',\n",
       " 'peeple',\n",
       " 'prank',\n",
       " 'relative',\n",
       " 'rodgers',\n",
       " 'serenity',\n",
       " 'smackdown',\n",
       " 'smartphone',\n",
       " 'soooooo',\n",
       " 'stoked',\n",
       " 'violation',\n",
       " '▸',\n",
       " 'achievement',\n",
       " 'alphabet',\n",
       " 'badass',\n",
       " 'bbad',\n",
       " 'communism',\n",
       " 'donut',\n",
       " 'gang',\n",
       " 'guessing',\n",
       " 'hoh',\n",
       " 'hunted',\n",
       " 'kalam',\n",
       " 'lasvegas',\n",
       " 'latenight',\n",
       " 'libya',\n",
       " 'mock',\n",
       " 'namaste',\n",
       " 'nextbanidea',\n",
       " 'nicknamed',\n",
       " 'objective',\n",
       " 'probe',\n",
       " 'published',\n",
       " 'remembering',\n",
       " 'resigns',\n",
       " 'saudiarabia',\n",
       " 'shoulda',\n",
       " 'smallbusiness',\n",
       " 'soo',\n",
       " 'subtweet',\n",
       " 'torture',\n",
       " 'viking',\n",
       " 'yemen',\n",
       " 'auburn',\n",
       " 'bitching',\n",
       " 'boehners',\n",
       " 'counterfeit',\n",
       " 'enforcement',\n",
       " 'eyeroll',\n",
       " 'gasp',\n",
       " 'hella',\n",
       " 'hometown',\n",
       " 'ironing',\n",
       " 'joined',\n",
       " 'mosque',\n",
       " 'ndtv',\n",
       " 'nuisance',\n",
       " 'organisation']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_not = result_df[result_df['Present in All classes'] == False].nlargest(300, 'Total Count')['Word'].tolist()\n",
    "top_words_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3a537c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['irony',\n",
       " 'ironic',\n",
       " 'news',\n",
       " 'im',\n",
       " 'love',\n",
       " 'like',\n",
       " 'people',\n",
       " 'get',\n",
       " 'amp',\n",
       " 'peace',\n",
       " 'day',\n",
       " 'late',\n",
       " 'drug',\n",
       " 'humor',\n",
       " 'education',\n",
       " 'one',\n",
       " 'politics',\n",
       " 'u',\n",
       " 'funny',\n",
       " 'time',\n",
       " 'good',\n",
       " 'know',\n",
       " 'lol',\n",
       " 'say',\n",
       " 'make',\n",
       " 'sarcastic',\n",
       " 'great',\n",
       " 'life',\n",
       " 'cant',\n",
       " 'today',\n",
       " 'gopdebate',\n",
       " 'right',\n",
       " 'see',\n",
       " 'work',\n",
       " 'really',\n",
       " 'new',\n",
       " 'go',\n",
       " 'would',\n",
       " 'oh',\n",
       " 'want',\n",
       " 'need',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'got',\n",
       " 'well',\n",
       " '’',\n",
       " 'way',\n",
       " 'never',\n",
       " 'guy',\n",
       " 'year',\n",
       " 'back',\n",
       " 'thats',\n",
       " 'going',\n",
       " 'much',\n",
       " 'look',\n",
       " 'night',\n",
       " 'via',\n",
       " 'thanks',\n",
       " 'man',\n",
       " 'take',\n",
       " 'woman',\n",
       " 'world',\n",
       " 'first',\n",
       " 'better',\n",
       " 'game',\n",
       " 'best',\n",
       " 'someone',\n",
       " 'last',\n",
       " 'fun',\n",
       " 'trump',\n",
       " 'sure',\n",
       " 'tweet',\n",
       " 'job',\n",
       " 'even',\n",
       " 'still',\n",
       " 'school',\n",
       " 'getting',\n",
       " 'come',\n",
       " 'let',\n",
       " 'always',\n",
       " 'show',\n",
       " 'could',\n",
       " 'twitter',\n",
       " 'yes',\n",
       " 'find',\n",
       " 'yet',\n",
       " 'week',\n",
       " 'another',\n",
       " 'yeah',\n",
       " 'mean',\n",
       " 'wait',\n",
       " 'call',\n",
       " 'ever',\n",
       " 'feel',\n",
       " 'start',\n",
       " 'gop',\n",
       " 'wow',\n",
       " 'nothing',\n",
       " 'he',\n",
       " 'said',\n",
       " 'use',\n",
       " 'many',\n",
       " 'kid',\n",
       " 'quote',\n",
       " 'thought',\n",
       " 'made',\n",
       " 'bad',\n",
       " 'everyone',\n",
       " 'design',\n",
       " 'next',\n",
       " 'friend',\n",
       " 'every',\n",
       " 'happy',\n",
       " 'hour',\n",
       " 'free',\n",
       " 'money',\n",
       " 'tell',\n",
       " 'morning',\n",
       " 'watching',\n",
       " 'keep',\n",
       " 'something',\n",
       " 'tech',\n",
       " '…',\n",
       " 'talk',\n",
       " 'big',\n",
       " 'live',\n",
       " 'girl',\n",
       " 'watch',\n",
       " 'fan',\n",
       " 'god',\n",
       " 'real',\n",
       " 'word',\n",
       " 'must',\n",
       " 'home',\n",
       " 'medium',\n",
       " 'ive',\n",
       " 'hate',\n",
       " 'team',\n",
       " 'video',\n",
       " 'help',\n",
       " 'play',\n",
       " 'actually',\n",
       " 'stop',\n",
       " 'na',\n",
       " 'internet',\n",
       " 'thank',\n",
       " 'read',\n",
       " 'black',\n",
       " 'white',\n",
       " 'post',\n",
       " 'nice',\n",
       " '😂',\n",
       " 'give',\n",
       " 'book',\n",
       " 'phone',\n",
       " 'tonight',\n",
       " 'car',\n",
       " 'joke',\n",
       " 'two',\n",
       " 'trying',\n",
       " 'rt',\n",
       " 'person',\n",
       " 'glad',\n",
       " 'name',\n",
       " 'run',\n",
       " 'country',\n",
       " 'also',\n",
       " 'put',\n",
       " 'end',\n",
       " 'coming',\n",
       " 'top',\n",
       " 'win',\n",
       " 'called',\n",
       " 'change',\n",
       " 'old',\n",
       " 'there',\n",
       " 'awesome',\n",
       " 'lot',\n",
       " 'talking',\n",
       " 'music',\n",
       " 'class',\n",
       " 'without',\n",
       " 'inspirational',\n",
       " 'party',\n",
       " 'social',\n",
       " 'done',\n",
       " '“',\n",
       " 'health',\n",
       " 'little',\n",
       " 'long',\n",
       " 'hope',\n",
       " 'please',\n",
       " 'state',\n",
       " 'guess',\n",
       " 'theyre',\n",
       " 'making',\n",
       " 'problem',\n",
       " 'business',\n",
       " 'shit',\n",
       " 'working',\n",
       " 'wrong',\n",
       " 'sign',\n",
       " 'service',\n",
       " 'gun',\n",
       " 'story',\n",
       " 'everything',\n",
       " 'hard',\n",
       " 'believe',\n",
       " 'food',\n",
       " 'song',\n",
       " 'sleep',\n",
       " '”',\n",
       " 'th',\n",
       " 'anyone',\n",
       " 'place',\n",
       " 'point',\n",
       " 'debate',\n",
       " 'may',\n",
       " 'saw',\n",
       " 'hey',\n",
       " 'using',\n",
       " 'playing',\n",
       " 'tv',\n",
       " 'war',\n",
       " 'haha',\n",
       " 'idea',\n",
       " 'maybe',\n",
       " 'else',\n",
       " 'family',\n",
       " 'looking',\n",
       " 'men',\n",
       " 'used',\n",
       " 'child',\n",
       " 'movie',\n",
       " 'tcot',\n",
       " 'r',\n",
       " 'around',\n",
       " 'writing',\n",
       " 'enough',\n",
       " 'football',\n",
       " 'india',\n",
       " 'w',\n",
       " 'support',\n",
       " 'saying',\n",
       " 'away',\n",
       " 'photo',\n",
       " 'true',\n",
       " 'american',\n",
       " 'gon',\n",
       " 'already',\n",
       " 'question',\n",
       " 'house',\n",
       " 'obama',\n",
       " 'care',\n",
       " 'p',\n",
       " 'since',\n",
       " 'america',\n",
       " 'yay',\n",
       " 'course',\n",
       " 'fact',\n",
       " 'truth',\n",
       " 'dog',\n",
       " 'high',\n",
       " 'totally',\n",
       " 'law',\n",
       " 'pay',\n",
       " 'ill',\n",
       " 'line',\n",
       " 'teacher',\n",
       " 'issue',\n",
       " 'lost',\n",
       " 'turn',\n",
       " 'sound',\n",
       " 'student',\n",
       " 'art',\n",
       " 'picture',\n",
       " 'police',\n",
       " 'republican',\n",
       " 'left',\n",
       " 'face',\n",
       " 'list',\n",
       " 'part',\n",
       " 'try',\n",
       " 'seems',\n",
       " 'anything',\n",
       " 'summer',\n",
       " 'email',\n",
       " 'tomorrow',\n",
       " 'cause',\n",
       " 'president',\n",
       " 'ad',\n",
       " 'boy',\n",
       " 'season',\n",
       " 'found',\n",
       " 'whats',\n",
       " 'reason',\n",
       " 'reading',\n",
       " 'check',\n",
       " 'hit',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_are = result_df[result_df['Present in All classes'] == True].nlargest(300, 'Total Count')['Word'].tolist()\n",
    "top_words_are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f795479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in top_words_are])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff0432df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0       aware dirty step staylight staywhite moralneeded  figurative\n",
       "1                       sarcasm understand diy artattack  figurative\n",
       "2      dailymail reader sensible shocker dailyfail in...  figurative\n",
       "3                                        feeling sarcasm  figurative\n",
       "4                                   probably missed text  figurative\n",
       "...                                                  ...         ...\n",
       "81403                 image heart childhood cool sarcasm     sarcasm\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm     sarcasm\n",
       "81405  wanted puberty letting apart itty bitty fuckin...     sarcasm\n",
       "81406  coverage fox special hidden harvest influence ...     sarcasm\n",
       "81407                                            sarcasm     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab0338",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "308d992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4202cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweets']\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c167076",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7035328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "635cd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce4c01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "classifier.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a6a3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92ceb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5455103795602506\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72293031",
   "metadata": {},
   "source": [
    "## COUNT vECTORIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6a3f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming you have the preprocessed tweet data and corresponding class labels stored in lists or arrays\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training data\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55e5b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the classifier (e.g., SVM)\n",
    "classifier = SVC()\n",
    "\n",
    "# Train the classifier on the count-based features and class labels\n",
    "classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "051ed7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5668836752241739\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  figurative       0.03      0.01      0.01      4179\n",
      "       irony       0.48      0.77      0.59      4276\n",
      "     regular       0.81      0.64      0.72      3696\n",
      "     sarcasm       0.64      0.86      0.73      4131\n",
      "\n",
      "    accuracy                           0.57     16282\n",
      "   macro avg       0.49      0.57      0.51     16282\n",
      "weighted avg       0.48      0.57      0.51     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have the true class labels stored in y_test\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a6ca3",
   "metadata": {},
   "source": [
    "### word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d721cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping={'figurative':0, 'irony':1, 'regular':2, 'sarcasm':3}\n",
    "df['class']=df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af336d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b287117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37236b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  class\n",
       "0       aware dirty step staylight staywhite moralneeded      0\n",
       "1                       sarcasm understand diy artattack      0\n",
       "2      dailymail reader sensible shocker dailyfail in...      0\n",
       "3                                        feeling sarcasm      0\n",
       "4                                   probably missed text      0\n",
       "...                                                  ...    ...\n",
       "81403                 image heart childhood cool sarcasm      3\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm      3\n",
       "81405  wanted puberty letting apart itty bitty fuckin...      3\n",
       "81406  coverage fox special hidden harvest influence ...      3\n",
       "81407                                            sarcasm      3\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc09786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48157474511730747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarth Naik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Tokenized tweets\n",
    "tokenized_tweets = [tweet.split() for tweet in df['tweets']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert tweets to average word embeddings\n",
    "tweet_embeddings = []\n",
    "for tweet in tokenized_tweets:\n",
    "    embeddings = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if embeddings:\n",
    "        tweet_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        tweet_embedding = np.zeros(100)  # Use zero vector if no word embeddings found\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Prepare data for training\n",
    "X = np.vstack(tweet_embeddings)\n",
    "y = df['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a77bf",
   "metadata": {},
   "source": [
    "## N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5717a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step staylight staywhite moralneeded</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm understand diy artattack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible shocker dailyfail in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling sarcasm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>image heart childhood cool sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>knewi universe lolmaybe date upon horizon sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>wanted puberty letting apart itty bitty fuckin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>coverage fox special hidden harvest influence ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>sarcasm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  class\n",
       "0       aware dirty step staylight staywhite moralneeded      0\n",
       "1                       sarcasm understand diy artattack      0\n",
       "2      dailymail reader sensible shocker dailyfail in...      0\n",
       "3                                        feeling sarcasm      0\n",
       "4                                   probably missed text      0\n",
       "...                                                  ...    ...\n",
       "81403                 image heart childhood cool sarcasm      3\n",
       "81404  knewi universe lolmaybe date upon horizon sarcasm      3\n",
       "81405  wanted puberty letting apart itty bitty fuckin...      3\n",
       "81406  coverage fox special hidden harvest influence ...      3\n",
       "81407                                            sarcasm      3\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "267b1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping={0:'figurative', 1:'irony', 2:'regular', 3:'sarcasm'}\n",
    "df['class']=df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1dbf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b218df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Count  Total Count\n",
      "(funny, sarcasm)     192       166797\n",
      "(right, sarcasm)     118       166797\n",
      "(gon, na)            107       166797\n",
      "(day, sarcasm)       104       166797\n",
      "(look, like)          98       166797\n",
      "...                  ...          ...\n",
      "(visited, mosque)      1       166797\n",
      "(never, visited)       1       166797\n",
      "(image, though)        1       166797\n",
      "(typo, image)          1       166797\n",
      "(rwc, 🏉)               1       166797\n",
      "\n",
      "[136432 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Select a particular class\n",
    "target_class = 'figurative'\n",
    "\n",
    "# Filter the dataset for the target class\n",
    "target_data = data[data['class'] == target_class]\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokenized_tweets = []\n",
    "for tweet in target_data['tweets']:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweets.append(tweet.split())\n",
    "\n",
    "# Generate bigrams\n",
    "tweet_bigrams = [list(bigrams(tweet_tokens)) for tweet_tokens in tokenized_tweets]\n",
    "\n",
    "# Flatten the list of bigrams\n",
    "all_bigrams = [bigram for tweet_bigram in tweet_bigrams for bigram in tweet_bigram]\n",
    "\n",
    "# Count the occurrences of each bigram\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "bigram_df = pd.DataFrame.from_dict(bigram_counts, orient='index', columns=['Count'])\n",
    "\n",
    "# Sort the DataFrame by the count column in descending order\n",
    "bigram_df = bigram_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Calculate the total count of bigrams in the target class\n",
    "total_count = bigram_df['Count'].sum()\n",
    "\n",
    "# Add a column with the total count for all classes\n",
    "bigram_df['Total Count'] = total_count\n",
    "\n",
    "print(bigram_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b952bdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(funny, sarcasm)</th>\n",
       "      <td>192</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(right, sarcasm)</th>\n",
       "      <td>118</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(gon, na)</th>\n",
       "      <td>107</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(day, sarcasm)</th>\n",
       "      <td>104</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(look, like)</th>\n",
       "      <td>98</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(visited, mosque)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(never, visited)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(image, though)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(typo, image)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(rwc, 🏉)</th>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136432 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Count  Total Count\n",
       "(funny, sarcasm)     192       166797\n",
       "(right, sarcasm)     118       166797\n",
       "(gon, na)            107       166797\n",
       "(day, sarcasm)       104       166797\n",
       "(look, like)          98       166797\n",
       "...                  ...          ...\n",
       "(visited, mosque)      1       166797\n",
       "(never, visited)       1       166797\n",
       "(image, though)        1       166797\n",
       "(typo, image)          1       166797\n",
       "(rwc, 🏉)               1       166797\n",
       "\n",
       "[136432 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f3c8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "bigram_df.to_excel('result2.xlsx', index_label='Bigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31cd00f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('funny', 'sarcasm')</td>\n",
       "      <td>192</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('right', 'sarcasm')</td>\n",
       "      <td>118</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('gon', 'na')</td>\n",
       "      <td>107</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('day', 'sarcasm')</td>\n",
       "      <td>104</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('look', 'like')</td>\n",
       "      <td>98</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136427</th>\n",
       "      <td>('visited', 'mosque')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136428</th>\n",
       "      <td>('never', 'visited')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136429</th>\n",
       "      <td>('image', 'though')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136430</th>\n",
       "      <td>('typo', 'image')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136431</th>\n",
       "      <td>('rwc', '🏉')</td>\n",
       "      <td>1</td>\n",
       "      <td>166797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136432 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Bigram  Count  Total Count\n",
       "0        ('funny', 'sarcasm')    192       166797\n",
       "1        ('right', 'sarcasm')    118       166797\n",
       "2               ('gon', 'na')    107       166797\n",
       "3          ('day', 'sarcasm')    104       166797\n",
       "4            ('look', 'like')     98       166797\n",
       "...                       ...    ...          ...\n",
       "136427  ('visited', 'mosque')      1       166797\n",
       "136428   ('never', 'visited')      1       166797\n",
       "136429    ('image', 'though')      1       166797\n",
       "136430      ('typo', 'image')      1       166797\n",
       "136431           ('rwc', '🏉')      1       166797\n",
       "\n",
       "[136432 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re2=pd.read_excel('result2.xlsx')\n",
    "re2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "527ff498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('funny', 'sarcasm')\",\n",
       " \"('right', 'sarcasm')\",\n",
       " \"('gon', 'na')\",\n",
       " \"('day', 'sarcasm')\",\n",
       " \"('look', 'like')\",\n",
       " \"('cant', 'wait')\",\n",
       " \"('im', 'sure')\",\n",
       " \"('fun', 'sarcasm')\",\n",
       " \"('social', 'medium')\",\n",
       " \"('last', 'night')\",\n",
       " \"('oh', 'irony')\",\n",
       " \"('great', 'sarcasm')\",\n",
       " \"('got', 'ta')\",\n",
       " \"('lol', 'sarcasm')\",\n",
       " \"('lol', 'irony')\",\n",
       " \"('time', 'sarcasm')\",\n",
       " \"('game', 'sarcasm')\",\n",
       " \"('work', 'sarcasm')\",\n",
       " \"('im', 'glad')\",\n",
       " \"('good', 'job')\",\n",
       " \"('sound', 'like')\",\n",
       " \"('better', 'sarcasm')\",\n",
       " \"('good', 'thing')\",\n",
       " \"('today', 'sarcasm')\",\n",
       " \"('job', 'sarcasm')\",\n",
       " \"('day', 'irony')\",\n",
       " \"('awesome', 'sarcasm')\",\n",
       " \"('yay', 'sarcasm')\",\n",
       " \"('love', 'sarcasm')\",\n",
       " \"('wan', 'na')\",\n",
       " \"('know', 'sarcasm')\",\n",
       " \"('lol', 'ironic')\",\n",
       " \"('year', 'sarcasm')\",\n",
       " \"('good', 'sarcasm')\",\n",
       " \"('well', 'done')\",\n",
       " \"('well', 'sarcasm')\",\n",
       " \"('feel', 'like')\",\n",
       " \"('irony', 'life')\",\n",
       " \"('year', 'ago')\",\n",
       " \"('today', 'irony')\",\n",
       " \"('love', 'people')\",\n",
       " \"('😂', 'irony')\",\n",
       " \"('life', 'sarcasm')\",\n",
       " \"('seems', 'like')\",\n",
       " \"('much', 'fun')\",\n",
       " \"('humor', 'sarcasm')\",\n",
       " \"('thank', 'god')\",\n",
       " \"('guy', 'sarcasm')\",\n",
       " \"('right', 'irony')\",\n",
       " \"('see', 'irony')\",\n",
       " \"('people', 'sarcasm')\",\n",
       " \"('tonight', 'sarcasm')\",\n",
       " \"('much', 'better')\",\n",
       " \"('ever', 'sarcasm')\",\n",
       " \"('anyone', 'else')\",\n",
       " \"('one', 'sarcasm')\",\n",
       " \"('make', 'sense')\",\n",
       " \"('life', 'irony')\",\n",
       " \"('ta', 'love')\",\n",
       " \"('last', 'year')\",\n",
       " \"('like', 'sarcasm')\",\n",
       " \"('cant', 'believe')\",\n",
       " \"('donald', 'trump')\",\n",
       " \"('sarcasm', 'lol')\",\n",
       " \"('cant', 'even')\",\n",
       " \"('ha', 'ha')\",\n",
       " \"('thing', 'sarcasm')\",\n",
       " \"('many', 'people')\",\n",
       " \"('climate', 'change')\",\n",
       " \"('kim', 'davis')\",\n",
       " \"('im', 'going')\",\n",
       " \"('one', 'day')\",\n",
       " \"('like', 'irony')\",\n",
       " \"('day', 'ironic')\",\n",
       " \"('great', 'day')\",\n",
       " \"('thanks', 'sarcasm')\",\n",
       " \"('tweet', 'sarcasm')\",\n",
       " \"('find', 'ironic')\",\n",
       " \"('shocked', 'sarcasm')\",\n",
       " \"('coming', 'sarcasm')\",\n",
       " \"('labor', 'day')\",\n",
       " \"('cant', 'get')\",\n",
       " \"('glad', 'see')\",\n",
       " \"('one', 'irony')\",\n",
       " \"('😒', 'sarcasm')\",\n",
       " \"('sarcasm', 'bb')\",\n",
       " \"('idea', 'sarcasm')\",\n",
       " \"('time', 'irony')\",\n",
       " \"('u', 'sarcasm')\",\n",
       " \"('life', 'ironic')\",\n",
       " \"('well', 'thats')\",\n",
       " \"('night', 'sarcasm')\",\n",
       " \"('looking', 'forward')\",\n",
       " \"('great', 'job')\",\n",
       " \"('im', 'shocked')\",\n",
       " \"('first', 'time')\",\n",
       " \"('fair', 'play')\",\n",
       " \"('😂', 'ironic')\",\n",
       " \"('sarcasm', 'gopdebate')\",\n",
       " \"('great', 'way')\",\n",
       " \"('twitter', 'irony')\",\n",
       " \"('people', 'like')\",\n",
       " \"('year', 'old')\",\n",
       " \"('irony', 'gopdebate')\",\n",
       " \"('wrong', 'sarcasm')\",\n",
       " \"('sense', 'sarcasm')\",\n",
       " \"('love', 'irony')\",\n",
       " \"('would', 'never')\",\n",
       " \"('problem', 'sarcasm')\",\n",
       " \"('independence', 'day')\",\n",
       " \"('get', 'better')\",\n",
       " \"('make', 'feel')\",\n",
       " \"('someone', 'else')\",\n",
       " \"('good', 'see')\",\n",
       " \"('really', 'good')\",\n",
       " \"('tweet', 'irony')\",\n",
       " \"('back', 'sarcasm')\",\n",
       " \"('😂', 'sarcasm')\",\n",
       " \"('get', 'sarcasm')\",\n",
       " \"('let', 'go')\",\n",
       " \"('best', 'sarcasm')\",\n",
       " \"('u', 'irony')\",\n",
       " \"('irony', 'lol')\",\n",
       " \"('sarcasm', 'humor')\",\n",
       " \"('pretty', 'sure')\",\n",
       " \"('via', 'irony')\",\n",
       " \"('nothing', 'like')\",\n",
       " \"('show', 'irony')\",\n",
       " \"('im', 'gon')\",\n",
       " \"('go', 'back')\",\n",
       " \"('week', 'sarcasm')\",\n",
       " \"('thing', 'irony')\",\n",
       " \"('customer', 'service')\",\n",
       " \"('nice', 'see')\",\n",
       " \"('every', 'time')\",\n",
       " \"('need', 'sarcasm')\",\n",
       " \"('say', 'sarcasm')\",\n",
       " \"('high', 'school')\",\n",
       " \"('great', 'start')\",\n",
       " \"('phone', 'irony')\",\n",
       " \"('go', 'sarcasm')\",\n",
       " \"('everyone', 'else')\",\n",
       " \"('cant', 'sleep')\",\n",
       " \"('im', 'still')\",\n",
       " \"('today', 'ironic')\",\n",
       " \"('yet', 'sarcasm')\",\n",
       " \"('done', 'sarcasm')\",\n",
       " \"('way', 'sarcasm')\",\n",
       " \"('human', 'right')\",\n",
       " \"('wait', 'see')\",\n",
       " \"('surprise', 'sarcasm')\",\n",
       " \"('sarcasm', 'sarcastic')\",\n",
       " \"('much', 'sarcasm')\",\n",
       " \"('first', 'day')\",\n",
       " \"('sure', 'sarcasm')\",\n",
       " \"('way', 'start')\",\n",
       " \"('know', 'irony')\",\n",
       " \"('oh', 'wait')\",\n",
       " \"('good', 'idea')\",\n",
       " \"('think', 'im')\",\n",
       " \"('morning', 'sarcasm')\",\n",
       " \"('next', 'week')\",\n",
       " \"('show', 'sarcasm')\",\n",
       " \"('oh', 'good')\",\n",
       " \"('job', 'irony')\",\n",
       " \"('work', 'irony')\",\n",
       " \"('team', 'sarcasm')\",\n",
       " \"('u', 'r')\",\n",
       " \"('world', 'sarcasm')\",\n",
       " \"('way', 'go')\",\n",
       " \"('im', 'watching')\",\n",
       " \"('sarcasm', 'funny')\",\n",
       " \"('everything', 'sarcasm')\",\n",
       " \"('ben', 'carson')\",\n",
       " \"('people', 'irony')\",\n",
       " \"('planned', 'parenthood')\",\n",
       " \"('gun', 'sarcasm')\",\n",
       " \"('good', 'time')\",\n",
       " \"('gun', 'control')\",\n",
       " \"('”', 'irony')\",\n",
       " \"('irony', 'ironic')\",\n",
       " \"('irony', '😂')\",\n",
       " \"('anyway', 'sarcasm')\",\n",
       " \"('would', 'love')\",\n",
       " \"('really', 'love')\",\n",
       " \"('wait', 'sarcasm')\",\n",
       " \"('get', 'irony')\",\n",
       " \"('irony', 'lost')\",\n",
       " \"('oh', 'yeah')\",\n",
       " \"('pretty', 'much')\",\n",
       " \"('west', 'ham')\",\n",
       " \"('week', 'irony')\",\n",
       " \"('back', 'work')\",\n",
       " \"('say', 'irony')\",\n",
       " \"('trying', 'get')\",\n",
       " \"('definition', 'irony')\",\n",
       " \"('oh', 'look')\",\n",
       " \"('really', 'sarcasm')\",\n",
       " \"('would', 'like')\",\n",
       " \"('ive', 'ever')\",\n",
       " \"('sarcasm', 'sarcasm')\",\n",
       " \"('think', 'sarcasm')\",\n",
       " \"('start', 'day')\",\n",
       " \"('sarcasm', 'good')\",\n",
       " \"('fan', 'sarcasm')\",\n",
       " \"('year', 'irony')\",\n",
       " \"('funny', 'irony')\",\n",
       " \"('nothing', 'better')\",\n",
       " \"('service', 'sarcasm')\",\n",
       " \"('believe', 'sarcasm')\",\n",
       " \"('really', 'want')\",\n",
       " \"('medium', 'irony')\",\n",
       " \"('sarcasm', '😂')\",\n",
       " \"('im', 'really')\",\n",
       " \"('every', 'day')\",\n",
       " \"('good', 'morning')\",\n",
       " \"('last', 'week')\",\n",
       " \"('even', 'though')\",\n",
       " \"('people', 'get')\",\n",
       " \"('getting', 'better')\",\n",
       " \"('read', 'irony')\",\n",
       " \"('want', 'see')\",\n",
       " \"('good', 'news')\",\n",
       " \"('oh', 'well')\",\n",
       " \"('back', 'irony')\",\n",
       " \"('haha', 'sarcasm')\",\n",
       " \"('like', 'im')\",\n",
       " \"('people', 'think')\",\n",
       " \"('world', 'irony')\",\n",
       " \"('know', 'im')\",\n",
       " \"('dontvote', 'sarcasm')\",\n",
       " \"('better', 'irony')\",\n",
       " \"('tv', 'show')\",\n",
       " \"('last', 'tweet')\",\n",
       " \"('make', 'sure')\",\n",
       " \"('win', 'sarcasm')\",\n",
       " \"('confederate', 'flag')\",\n",
       " \"('keep', 'getting')\",\n",
       " \"('woman', 'irony')\",\n",
       " \"('super', 'bowl')\",\n",
       " \"('work', 'today')\",\n",
       " \"('would', 'say')\",\n",
       " \"('alanis', 'morissette')\",\n",
       " \"('always', 'fun')\",\n",
       " \"('news', 'sarcasm')\",\n",
       " \"('change', 'irony')\",\n",
       " \"('smh', 'sarcasm')\",\n",
       " \"('good', 'day')\",\n",
       " \"('sarcasm', 'hashtag')\",\n",
       " \"('sarcasm', 'tag')\",\n",
       " \"('season', 'sarcasm')\",\n",
       " \"('sarcasm', 'cdnpoli')\",\n",
       " \"('see', 'coming')\",\n",
       " \"('night', 'irony')\",\n",
       " \"('tomorrow', 'sarcasm')\",\n",
       " \"('pretty', 'good')\",\n",
       " \"('shocking', 'sarcasm')\",\n",
       " \"('knew', 'sarcasm')\",\n",
       " \"('india', 'irony')\",\n",
       " \"('though', 'sarcasm')\",\n",
       " \"('im', 'excited')\",\n",
       " \"('white', 'people')\",\n",
       " \"('football', 'sarcasm')\",\n",
       " \"('im', 'sorry')\",\n",
       " \"('issue', 'irony')\",\n",
       " \"('never', 'get')\",\n",
       " \"('friend', 'sarcasm')\",\n",
       " \"('irony', 'auspol')\",\n",
       " \"('go', 'wrong')\",\n",
       " \"('ashley', 'madison')\",\n",
       " \"('weekend', 'sarcasm')\",\n",
       " \"('tcot', 'p')\",\n",
       " \"('bad', 'sarcasm')\",\n",
       " \"('one', 'ironic')\",\n",
       " \"('irony', 'irony')\",\n",
       " \"('else', 'sarcasm')\",\n",
       " \"('see', 'sarcasm')\",\n",
       " \"('put', 'sarcasm')\",\n",
       " \"('forgot', 'sarcasm')\",\n",
       " \"('game', 'irony')\",\n",
       " \"('people', 'say')\",\n",
       " \"('first', 'thing')\",\n",
       " \"('say', 'guy')\",\n",
       " \"('class', 'sarcasm')\",\n",
       " \"('want', 'sarcasm')\",\n",
       " \"('sarcasm', 'joke')\",\n",
       " \"('much', 'irony')\",\n",
       " \"('going', 'great')\",\n",
       " \"('football', 'irony')\",\n",
       " \"('love', 'getting')\",\n",
       " \"('even', 'know')\",\n",
       " \"('get', 'back')\",\n",
       " \"('😑', 'sarcasm')\",\n",
       " \"('use', 'sarcasm')\",\n",
       " \"('everyone', 'know')\",\n",
       " \"('saw', 'coming')\",\n",
       " \"('u', 'know')\",\n",
       " \"('love', 'u')\",\n",
       " \"('home', 'irony')\",\n",
       " \"('man', 'sarcasm')\",\n",
       " \"('understand', 'sarcasm')\",\n",
       " \"('start', 'sarcasm')\",\n",
       " \"('friend', 'irony')\",\n",
       " \"('sarcasm', 'quote')\",\n",
       " \"('one', 'thing')\",\n",
       " \"('well', 'irony')\",\n",
       " \"('bumper', 'sticker')\",\n",
       " \"('long', 'time')\",\n",
       " \"('look', 'good')\",\n",
       " \"('people', 'know')\",\n",
       " \"('something', 'sarcasm')\",\n",
       " \"('bb', 'sarcasm')\",\n",
       " \"('could', 'go')\",\n",
       " \"('way', 'irony')\",\n",
       " \"('he', 'going')\",\n",
       " \"('gopdebate', 'irony')\",\n",
       " \"('come', 'back')\",\n",
       " \"('much', 'love')\",\n",
       " \"('yes', 'im')\",\n",
       " \"('nice', 'sarcasm')\",\n",
       " \"('next', 'year')\",\n",
       " \"('year', 'later')\",\n",
       " \"('cool', 'sarcasm')\",\n",
       " \"('excited', 'sarcasm')\",\n",
       " \"('true', 'sarcasm')\",\n",
       " \"('like', 'ironic')\",\n",
       " \"('irony', 'rt')\",\n",
       " \"('fan', 'irony')\",\n",
       " \"('tonight', 'irony')\",\n",
       " \"('irony', 'cdnpoli')\",\n",
       " \"('ironic', 'irony')\",\n",
       " \"('lowest', 'form')\",\n",
       " \"('way', 'end')\",\n",
       " \"('friday', 'night')\",\n",
       " \"('ultimate', 'irony')\",\n",
       " \"('irony', 'india')\",\n",
       " \"('best', 'way')\",\n",
       " \"('irony', 'best')\",\n",
       " \"('twitter', 'ironic')\",\n",
       " \"('another', 'day')\",\n",
       " \"('country', 'irony')\",\n",
       " \"('best', 'friend')\",\n",
       " \"('red', 'card')\",\n",
       " \"('shit', 'sarcasm')\",\n",
       " \"('irony', 'much')\",\n",
       " \"('form', 'wit')\",\n",
       " \"('funny', 'sarcastic')\",\n",
       " \"('joy', 'sarcasm')\",\n",
       " \"('question', 'sarcasm')\",\n",
       " \"('away', 'sarcasm')\",\n",
       " \"('deal', 'sarcasm')\",\n",
       " \"('people', 'understand')\",\n",
       " \"('im', 'trying')\",\n",
       " \"('irony', 'fun')\",\n",
       " \"('power', 'irony')\",\n",
       " \"('black', 'people')\",\n",
       " \"('today', 'going')\",\n",
       " \"('day', 'work')\",\n",
       " \"('cant', 'tell')\",\n",
       " \"('yr', 'old')\",\n",
       " \"('watch', 'sarcasm')\",\n",
       " \"('word', 'sarcasm')\",\n",
       " \"('month', 'sarcasm')\",\n",
       " \"('come', 'sarcasm')\",\n",
       " \"('sleep', 'irony')\",\n",
       " \"('gopdebate', 'sarcasm')\",\n",
       " \"('twitter', 'account')\",\n",
       " \"('id', 'rather')\",\n",
       " \"('first', 'sarcasm')\",\n",
       " \"('wedding', 'day')\",\n",
       " \"('ironic', 'lol')\",\n",
       " \"('happy', 'sarcasm')\",\n",
       " \"('far', 'sarcasm')\",\n",
       " \"('im', 'getting')\",\n",
       " \"('joke', 'irony')\",\n",
       " \"('twitter', 'sarcasm')\",\n",
       " \"('going', 'get')\",\n",
       " \"('day', 'today')\",\n",
       " \"('word', 'irony')\",\n",
       " \"('anything', 'sarcasm')\",\n",
       " \"('missed', 'sarcasm')\",\n",
       " \"('know', 'whats')\",\n",
       " \"('irony', 'finest')\",\n",
       " \"('call', 'sarcasm')\",\n",
       " \"('stuff', 'sarcasm')\",\n",
       " \"('europa', 'league')\",\n",
       " \"('nothing', 'say')\",\n",
       " \"('ive', 'never')\",\n",
       " \"('sarcasm', 'rt')\",\n",
       " \"('school', 'sarcasm')\",\n",
       " \"('school', 'irony')\",\n",
       " \"('right', 'wing')\",\n",
       " \"('one', 'person')\",\n",
       " \"('know', 'would')\",\n",
       " \"('seriously', 'sarcasm')\",\n",
       " \"('cant', 'find')\",\n",
       " \"('wearing', 'shirt')\",\n",
       " \"('glad', 'got')\",\n",
       " \"('go', 'ahead')\",\n",
       " \"('want', 'go')\",\n",
       " \"('ive', 'got')\",\n",
       " \"('white', 'house')\",\n",
       " \"('funny', 'thing')\",\n",
       " \"('sarcasm', '😒')\",\n",
       " \"('hashtag', 'sarcasm')\",\n",
       " \"('add', 'sarcasm')\",\n",
       " \"('change', 'sarcasm')\",\n",
       " \"('raw', 'sarcasm')\",\n",
       " \"('oh', 'dear')\",\n",
       " \"('one', 'best')\",\n",
       " \"('god', 'irony')\",\n",
       " \"('woman', 'right')\",\n",
       " \"('mean', 'like')\",\n",
       " \"('people', 'complaining')\",\n",
       " \"('cant', 'make')\",\n",
       " \"('face', 'irony')\",\n",
       " \"('point', 'irony')\",\n",
       " \"('get', 'enough')\",\n",
       " \"('lot', 'sarcasm')\",\n",
       " \"('im', 'sarcastic')\",\n",
       " \"('make', 'want')\",\n",
       " \"('get', 'rid')\",\n",
       " \"('want', 'irony')\",\n",
       " \"('irony', 'tcot')\",\n",
       " \"('prime', 'minister')\",\n",
       " \"('others', 'irony')\",\n",
       " \"('go', 'home')\",\n",
       " \"('cell', 'phone')\",\n",
       " \"('dog', 'day')\",\n",
       " \"('parking', 'lot')\",\n",
       " \"('working', 'sarcasm')\",\n",
       " \"('free', 'speech')\",\n",
       " \"('cant', 'handle')\",\n",
       " \"('last', 'day')\",\n",
       " \"('exciting', 'sarcasm')\",\n",
       " \"('money', 'irony')\",\n",
       " \"('never', 'happens')\",\n",
       " \"('work', 'day')\",\n",
       " \"('money', 'sarcasm')\",\n",
       " \"('fault', 'sarcasm')\",\n",
       " \"('ever', 'seen')\",\n",
       " \"('wow', 'sarcasm')\",\n",
       " \"('time', 'get')\",\n",
       " \"('right', 'ironic')\",\n",
       " \"('make', 'people')\",\n",
       " \"('😂😂', 'irony')\",\n",
       " \"('sarcasm', 'raw')\",\n",
       " \"('na', 'get')\",\n",
       " \"('life', 'funny')\",\n",
       " \"('thought', 'sarcasm')\",\n",
       " \"('funny', 'way')\",\n",
       " \"('safe', 'sarcasm')\",\n",
       " \"('people', 'want')\",\n",
       " \"('irony', 'funny')\",\n",
       " \"('irony', 'hypocrisy')\",\n",
       " \"('place', 'irony')\",\n",
       " \"('guess', 'sarcasm')\",\n",
       " \"('talk', 'irony')\",\n",
       " \"('love', 'love')\",\n",
       " \"('best', 'part')\",\n",
       " \"('funny', 'humor')\",\n",
       " \"('one', 'time')\",\n",
       " \"('yeah', 'thats')\",\n",
       " \"('happen', 'sarcasm')\",\n",
       " \"('thanks', 'lot')\",\n",
       " \"('good', 'luck')\",\n",
       " \"('know', 'much')\",\n",
       " \"('see', 'people')\",\n",
       " \"('yeah', 'im')\",\n",
       " \"('get', 'see')\",\n",
       " \"('quote', 'sarcasm')\",\n",
       " \"('almost', 'got')\",\n",
       " \"('favorite', 'sarcasm')\",\n",
       " \"('people', 'still')\",\n",
       " \"('u', 'want')\",\n",
       " \"('brilliant', 'sarcasm')\",\n",
       " \"('else', 'find')\",\n",
       " \"('cant', 'afford')\",\n",
       " \"('ago', 'irony')\",\n",
       " \"('wrong', 'irony')\",\n",
       " \"('course', 'sarcasm')\",\n",
       " \"('done', 'irony')\",\n",
       " \"('amp', 'sarcasm')\",\n",
       " \"('never', 'heard')\",\n",
       " \"('politics', 'irony')\",\n",
       " \"('irony', 'elxn')\",\n",
       " \"('irony', '…')\",\n",
       " \"('wish', 'could')\",\n",
       " \"('america', 'sarcasm')\",\n",
       " \"('thats', 'great')\",\n",
       " \"('would', 'sarcasm')\",\n",
       " \"('favorite', 'thing')\",\n",
       " \"('party', 'irony')\",\n",
       " \"('see', 'one')\",\n",
       " \"('bill', 'sarcasm')\",\n",
       " \"('feel', 'good')\",\n",
       " \"('irony', 'cnndebate')\",\n",
       " \"('sarcasm', 'im')\",\n",
       " \"('th', 'independence')\",\n",
       " \"('think', 'irony')\",\n",
       " \"('smoking', 'irony')\",\n",
       " \"('well', 'im')\",\n",
       " \"('ive', 'seen')\",\n",
       " \"('irony', 'via')\",\n",
       " \"('love', 'waking')\",\n",
       " \"('help', 'sarcasm')\",\n",
       " \"('people', 'love')\",\n",
       " \"('shirt', 'irony')\",\n",
       " \"('go', 'irony')\",\n",
       " \"('day', 'get')\",\n",
       " \"('nosmoking', 'smoking')\",\n",
       " \"('make', 'u')\",\n",
       " \"('landlord', 'ultimate')\",\n",
       " \"('fun', 'landlord')\",\n",
       " \"('license', 'plate')\",\n",
       " \"('landlord', 'nosmoking')\",\n",
       " \"('come', 'irony')\",\n",
       " \"('could', 'say')\",\n",
       " \"('better', 'better')\",\n",
       " \"('would', 'make')\",\n",
       " \"('na', 'go')\",\n",
       " \"('ha', 'irony')\",\n",
       " \"('there', 'nothing')\",\n",
       " \"('sarcasm', 'p')\",\n",
       " \"('u', 'see')\",\n",
       " \"('one', 'want')\",\n",
       " \"('people', 'would')\",\n",
       " \"('hillary', 'clinton')\",\n",
       " \"('creative', 'sarcasm')\",\n",
       " \"('woman', 'sarcasm')\",\n",
       " \"('thats', 'irony')\",\n",
       " \"('president', 'irony')\",\n",
       " \"('gop', 'debate')\",\n",
       " \"('sarcastic', 'sarcasm')\",\n",
       " \"('truth', 'sarcasm')\",\n",
       " \"('never', 'sarcasm')\",\n",
       " \"('fox', 'news')\",\n",
       " \"('oh', 'im')\",\n",
       " \"('week', 'ago')\",\n",
       " \"('hard', 'work')\",\n",
       " \"('irony', 'karma')\",\n",
       " \"('tho', 'sarcasm')\",\n",
       " \"('kid', 'irony')\",\n",
       " \"('human', 'sarcasm')\",\n",
       " \"('last', 'name')\",\n",
       " \"('taylor', 'swift')\",\n",
       " \"('coach', 'sarcasm')\",\n",
       " \"('breaking', 'news')\",\n",
       " \"('every', 'single')\",\n",
       " \"('head', 'sarcasm')\",\n",
       " \"('oh', 'love')\",\n",
       " \"('home', 'sarcasm')\",\n",
       " \"('would', 'get')\",\n",
       " \"('sarcasm', 'love')\",\n",
       " \"('genius', 'sarcasm')\",\n",
       " \"('debate', 'irony')\",\n",
       " \"('really', 'irony')\",\n",
       " \"('president', 'sarcasm')\",\n",
       " \"('acting', 'like')\",\n",
       " \"('people', 'believe')\",\n",
       " \"('lovely', 'sarcasm')\",\n",
       " \"('need', 'get')\",\n",
       " \"('great', 'idea')\",\n",
       " \"('😂😂😂', 'irony')\",\n",
       " \"('country', 'sarcasm')\",\n",
       " \"('like', 'great')\",\n",
       " \"('sarcasm', 'tcot')\",\n",
       " \"('enough', 'sarcasm')\",\n",
       " \"('irony', 'alert')\",\n",
       " \"('royal', 'sarcasm')\",\n",
       " \"('kill', 'people')\",\n",
       " \"('thats', 'good')\",\n",
       " \"('people', 'talk')\",\n",
       " \"('irony', 'one')\",\n",
       " \"('😂😂😂', 'sarcasm')\",\n",
       " \"('shit', 'irony')\",\n",
       " \"('could', 'possibly')\",\n",
       " \"('cant', 'help')\",\n",
       " \"('running', 'late')\",\n",
       " \"('beautiful', 'day')\",\n",
       " \"('buy', 'new')\",\n",
       " \"('work', 'ironic')\",\n",
       " \"('im', 'pretty')\",\n",
       " \"('white', 'guy')\",\n",
       " \"('one', 'week')\",\n",
       " \"('weight', 'loss')\",\n",
       " \"('make', 'perfect')\",\n",
       " \"('move', 'sarcasm')\",\n",
       " \"('love', 'seeing')\",\n",
       " \"('issue', 'sarcasm')\",\n",
       " \"('sarcastic', 'people')\",\n",
       " \"('like', 'one')\",\n",
       " \"('new', 'york')\",\n",
       " \"('people', 'call')\",\n",
       " \"('would', 'need')\",\n",
       " \"('trump', 'irony')\",\n",
       " \"('make', 'laugh')\",\n",
       " \"('tell', 'im')\",\n",
       " \"('helpful', 'sarcasm')\",\n",
       " \"('united', 'state')\",\n",
       " \"('perfect', 'sense')\",\n",
       " \"('irony', 'bb')\",\n",
       " \"('feel', 'better')\",\n",
       " \"('people', 'go')\",\n",
       " \"('terrible', 'sarcasm')\",\n",
       " \"('😏', 'sarcasm')\",\n",
       " \"('better', 'way')\",\n",
       " \"('get', 'mad')\",\n",
       " \"('coffee', 'shop')\",\n",
       " \"('irony', 'people')\",\n",
       " \"('something', 'irony')\",\n",
       " \"('real', 'life')\",\n",
       " \"('calling', 'someone')\",\n",
       " \"('hour', 'irony')\",\n",
       " \"('amazing', 'sarcasm')\",\n",
       " \"('irony', 'hypocrite')\",\n",
       " \"('like', 'watching')\",\n",
       " \"('work', 'hard')\",\n",
       " \"('make', 'fun')\",\n",
       " \"('could', 'get')\",\n",
       " \"('let', 'u')\",\n",
       " \"('irony', 'amp')\",\n",
       " \"('heard', 'sarcasm')\",\n",
       " \"('know', 'love')\",\n",
       " \"('wait', 'go')\",\n",
       " \"('im', 'tired')\",\n",
       " \"('go', 'work')\",\n",
       " \"('shame', 'sarcasm')\",\n",
       " \"('must', 'true')\",\n",
       " \"('sarcastic', 'tweet')\",\n",
       " \"('even', 'get')\",\n",
       " \"('lmao', 'sarcasm')\",\n",
       " \"('yet', 'another')\",\n",
       " \"('look', 'great')\",\n",
       " \"('paradox', 'irony')\",\n",
       " \"('sense', 'humor')\",\n",
       " \"('last', 'time')\",\n",
       " \"('boost', 'creativity')\",\n",
       " \"('cant', 'stop')\",\n",
       " \"('like', 'good')\",\n",
       " \"('plan', 'sarcasm')\",\n",
       " \"('sunday', 'sarcasm')\",\n",
       " \"('away', 'irony')\",\n",
       " \"('ive', 'heard')\",\n",
       " \"('around', 'irony')\",\n",
       " \"('sense', 'humour')\",\n",
       " \"('sarcasm', 'like')\",\n",
       " \"('humour', 'sarcasm')\",\n",
       " \"('well', 'least')\",\n",
       " \"('think', 'ironic')\",\n",
       " \"('company', 'irony')\",\n",
       " \"('thing', 'like')\",\n",
       " \"('get', 'hurt')\",\n",
       " \"('yet', 'im')\",\n",
       " \"('decision', 'sarcasm')\",\n",
       " \"('really', 'appreciate')\",\n",
       " \"('morning', 'irony')\",\n",
       " \"('looked', 'like')\",\n",
       " \"('michael', 'vick')\",\n",
       " \"('know', 'he')\",\n",
       " \"('sarcasm', 'seriously')\",\n",
       " \"('thank', 'sarcasm')\",\n",
       " \"('national', 'dog')\",\n",
       " \"('point', 'sarcasm')\",\n",
       " \"('😂😂', 'sarcasm')\",\n",
       " \"('racist', 'sarcasm')\",\n",
       " \"('would', 'know')\",\n",
       " \"('sarcasm', 'elxn')\",\n",
       " \"('sarcasm', '😐')\",\n",
       " \"('u', 'cant')\",\n",
       " \"('best', 'thing')\",\n",
       " \"('level', 'sarcasm')\",\n",
       " \"('example', 'irony')\",\n",
       " \"('really', 'need')\",\n",
       " \"('always', 'good')\",\n",
       " \"('😊', 'sarcasm')\",\n",
       " \"('appreciate', 'sarcasm')\",\n",
       " \"('quote', 'irony')\",\n",
       " \"('let', 'know')\",\n",
       " \"('alanis', 'morrisette')\",\n",
       " \"('trump', 'sarcasm')\",\n",
       " \"('matter', 'sarcasm')\",\n",
       " \"('yeah', 'sarcasm')\",\n",
       " \"('sarcasm', 'satire')\",\n",
       " \"('really', 'really')\",\n",
       " \"('song', 'ironic')\",\n",
       " \"('call', 'irony')\",\n",
       " \"('called', 'sarcasm')\",\n",
       " \"('fell', 'asleep')\",\n",
       " \"('people', 'actually')\",\n",
       " \"('global', 'warming')\",\n",
       " \"('thing', 'ironic')\",\n",
       " \"('scott', 'walker')\",\n",
       " \"('ya', 'think')\",\n",
       " \"('ok', 'sarcasm')\",\n",
       " \"('sarcasm', 'font')\",\n",
       " \"('game', 'ironic')\",\n",
       " \"('sarcasm', 'kinda')\",\n",
       " \"('yes', 'want')\",\n",
       " \"('stupid', 'irony')\",\n",
       " \"('say', 'something')\",\n",
       " \"('time', 'ironic')\",\n",
       " \"('really', 'well')\",\n",
       " \"('cant', 'feel')\",\n",
       " \"('labour', 'party')\",\n",
       " \"('sarcasm', 'people')\",\n",
       " \"('go', 'figure')\",\n",
       " \"('fun', 'fact')\",\n",
       " \"('finally', 'get')\",\n",
       " \"('hour', 'later')\",\n",
       " \"('around', 'sarcasm')\",\n",
       " \"('like', 'everyone')\",\n",
       " \"('road', 'irony')\",\n",
       " \"('last', 'minute')\",\n",
       " \"('player', 'sent')\",\n",
       " \"('call', 'someone')\",\n",
       " \"('class', 'irony')\",\n",
       " \"('last', 'one')\",\n",
       " \"('big', 'deal')\",\n",
       " \"('bad', 'thing')\",\n",
       " \"('bit', 'sarcasm')\",\n",
       " \"('love', 'im')\",\n",
       " \"('blue', 'jay')\",\n",
       " \"('sad', 'irony')\",\n",
       " \"('good', 'one')\",\n",
       " \"('comment', 'irony')\",\n",
       " \"('run', 'irony')\",\n",
       " \"('white', 'men')\",\n",
       " \"('im', 'like')\",\n",
       " \"('wow', 'really')\",\n",
       " \"('blessed', 'sarcasm')\",\n",
       " \"('girl', 'irony')\",\n",
       " \"('important', 'sarcasm')\",\n",
       " \"('😅', 'sarcasm')\",\n",
       " \"('cute', 'sarcasm')\",\n",
       " \"('made', 'laugh')\",\n",
       " \"('new', 'quote')\",\n",
       " \"('wonder', 'sarcasm')\",\n",
       " \"('thats', 'funny')\",\n",
       " \"('government', 'sarcasm')\",\n",
       " \"('know', 'really')\",\n",
       " \"('needed', 'sarcasm')\",\n",
       " \"('hour', 'sleep')\",\n",
       " \"('coming', 'back')\",\n",
       " \"('say', 'want')\",\n",
       " \"('saturday', 'night')\",\n",
       " \"('stop', 'talking')\",\n",
       " \"('man', 'irony')\",\n",
       " \"('middle', 'east')\",\n",
       " \"('wonderful', 'sarcasm')\",\n",
       " \"('yet', 'still')\",\n",
       " \"('cant', 'say')\",\n",
       " \"('offense', 'sarcasm')\",\n",
       " \"('boy', 'sarcasm')\",\n",
       " \"('house', 'sarcasm')\",\n",
       " \"('amp', 'get')\",\n",
       " \"('love', 'life')\",\n",
       " \"('football', 'game')\",\n",
       " \"('happened', 'sarcasm')\",\n",
       " \"('ironic', 'funny')\",\n",
       " \"('christian', 'irony')\",\n",
       " \"('still', 'waiting')\",\n",
       " \"('gay', 'marriage')\",\n",
       " \"('back', 'day')\",\n",
       " \"('win', 'irony')\",\n",
       " \"('match', 'irony')\",\n",
       " \"('humour', 'irony')\",\n",
       " \"('smart', 'sarcasm')\",\n",
       " \"('ever', 'irony')\",\n",
       " \"('already', 'sarcasm')\",\n",
       " \"('reading', 'article')\",\n",
       " \"('else', 'see')\",\n",
       " \"('getting', 'rid')\",\n",
       " \"('nothing', 'irony')\",\n",
       " \"('ok', 'irony')\",\n",
       " \"('alive', 'sarcasm')\",\n",
       " \"('duh', 'sarcasm')\",\n",
       " \"('hilarious', 'irony')\",\n",
       " \"('way', 'get')\",\n",
       " \"('used', 'sarcasm')\",\n",
       " \"('book', 'irony')\",\n",
       " \"('season', 'irony')\",\n",
       " \"('red', 'light')\",\n",
       " \"('ironic', 'think')\",\n",
       " \"('might', 'well')\",\n",
       " \"('ironic', '😂')\",\n",
       " \"('anyone', 'sarcasm')\",\n",
       " \"('thanks', 'help')\",\n",
       " \"('fall', 'asleep')\",\n",
       " \"('want', 'get')\",\n",
       " \"('people', 'cant')\",\n",
       " \"('oh', 'joy')\",\n",
       " \"('sarcasm', 'via')\",\n",
       " \"('something', 'like')\",\n",
       " \"('let', 'get')\",\n",
       " \"('true', 'irony')\",\n",
       " \"('hilarious', 'sarcasm')\",\n",
       " \"('men', 'irony')\",\n",
       " \"('bad', 'guy')\",\n",
       " \"('time', 'management')\",\n",
       " \"('😉', 'sarcasm')\",\n",
       " \"('life', 'matter')\",\n",
       " \"('added', 'sarcasm')\",\n",
       " \"('fantasy', 'football')\",\n",
       " \"('irony', 'u')\",\n",
       " \"('id', 'like')\",\n",
       " \"('profile', 'pic')\",\n",
       " \"('irony', 'many')\",\n",
       " \"('reading', 'book')\",\n",
       " \"('humor', 'irony')\",\n",
       " \"('stock', 'market')\",\n",
       " \"('ironic', '😂😂')\",\n",
       " \"('people', 'make')\",\n",
       " \"('life', 'great')\",\n",
       " \"('vote', 'sarcasm')\",\n",
       " \"('oh', 'goody')\",\n",
       " \"('hypocrisy', 'irony')\",\n",
       " \"('today', 'im')\",\n",
       " \"('love', 'watching')\",\n",
       " \"('pay', 'bill')\",\n",
       " \"('white', 'person')\",\n",
       " \"('person', 'sarcasm')\",\n",
       " \"('theyre', 'going')\",\n",
       " \"('men', 'sarcasm')\",\n",
       " \"('irony', 'vmas')\",\n",
       " \"('really', 'help')\",\n",
       " \"('play', 'irony')\",\n",
       " \"('hard', 'believe')\",\n",
       " \"('year', 'ironic')\",\n",
       " \"('happy', 'birthday')\",\n",
       " \"('day', 'weekend')\",\n",
       " \"('great', 'see')\",\n",
       " \"('super', 'excited')\",\n",
       " \"('p', 'sarcasm')\",\n",
       " \"('sarcasm', 'nfl')\",\n",
       " \"('read', 'sarcasm')\",\n",
       " \"('ago', 'today')\",\n",
       " \"('look', 'bad')\",\n",
       " \"('new', 'one')\",\n",
       " \"('funny', 'ironic')\",\n",
       " \"('monday', 'sarcasm')\",\n",
       " \"('another', 'one')\",\n",
       " \"('never', 'work')\",\n",
       " \"('dream', 'meeting')\",\n",
       " \"('via', 'sarcasm')\",\n",
       " \"('man', 'dream')\",\n",
       " \"('hahaha', 'sarcasm')\",\n",
       " \"('smh', 'irony')\",\n",
       " \"('every', 'year')\",\n",
       " \"('still', 'cant')\",\n",
       " \"('sarcasm', 'wtf')\",\n",
       " \"('awkward', 'moment')\",\n",
       " \"('someone', 'like')\",\n",
       " \"('draft', 'pick')\",\n",
       " \"('feel', 'face')\",\n",
       " \"('tell', 'u')\",\n",
       " \"('story', 'sarcasm')\",\n",
       " \"('shirt', 'say')\",\n",
       " \"('racist', 'irony')\",\n",
       " \"('many', 'time')\",\n",
       " \"('work', 'well')\",\n",
       " \"('one', 'know')\",\n",
       " \"('glad', 'im')\",\n",
       " \"('ball', 'sarcasm')\",\n",
       " \"('amp', 'im')\",\n",
       " \"('muslim', 'irony')\",\n",
       " \"('service', 'irony')\",\n",
       " \"('law', 'irony')\",\n",
       " \"('food', 'irony')\",\n",
       " \"('day', 'summer')\",\n",
       " \"('let', 'see')\",\n",
       " \"('say', 'anything')\",\n",
       " \"('bring', 'back')\",\n",
       " \"('funny', 'lol')\",\n",
       " \"('read', 'article')\",\n",
       " \"('great', 'news')\",\n",
       " \"('next', 'day')\",\n",
       " \"('little', 'bit')\",\n",
       " \"('sarcasm', 'work')\",\n",
       " \"('sarcasm', 'make')\",\n",
       " \"('hate', 'people')\",\n",
       " \"('exactly', 'like')\",\n",
       " \"('hour', 'work')\",\n",
       " \"('end', 'world')\",\n",
       " \"('notreally', 'sarcasm')\",\n",
       " \"('oh', 'yea')\",\n",
       " \"('omg', 'sarcasm')\",\n",
       " \"('play', 'league')\",\n",
       " \"('make', 'much')\",\n",
       " \"('world', 'live')\",\n",
       " \"('thats', 'right')\",\n",
       " \"('medium', 'sarcasm')\",\n",
       " \"('would', 'thought')\",\n",
       " \"('ad', 'irony')\",\n",
       " \"('church', 'irony')\",\n",
       " \"('people', 'never')\",\n",
       " \"('sleep', 'ironic')\",\n",
       " \"('someone', 'el')\",\n",
       " \"('make', 'irony')\",\n",
       " \"('right', 'next')\",\n",
       " \"('ball', 'irony')\",\n",
       " \"('auspol', 'irony')\",\n",
       " \"('thank', 'goodness')\",\n",
       " \"('note', 'self')\",\n",
       " \"('wait', 'get')\",\n",
       " \"('p', 'irony')\",\n",
       " \"('technology', 'irony')\",\n",
       " \"('thats', 'real')\",\n",
       " \"('food', 'sarcasm')\",\n",
       " \"('great', 'time')\",\n",
       " \"('end', 'irony')\",\n",
       " \"('room', 'sarcasm')\",\n",
       " \"('oh', 'thats')\",\n",
       " \"('get', 'dumped')\",\n",
       " \"('dumped', 'acting')\",\n",
       " \"('ironic', 'thing')\",\n",
       " \"('funny', 'people')\",\n",
       " \"('would', 'think')\",\n",
       " \"('dont', 'know')\",\n",
       " \"('know', 'u')\",\n",
       " \"('day', 'amp')\",\n",
       " \"('minute', 'irony')\",\n",
       " \"('good', 'irony')\",\n",
       " \"('last', 'thing')\",\n",
       " \"('black', 'woman')\",\n",
       " \"('control', 'sarcasm')\",\n",
       " \"('never', 'thought')\",\n",
       " \"('making', 'fun')\",\n",
       " \"('need', 'gun')\",\n",
       " \"('huh', 'sarcasm')\",\n",
       " \"('cant', 'go')\",\n",
       " \"('im', 'happy')\",\n",
       " \"('journalism', 'sarcasm')\",\n",
       " \"('irony', 'proofreading')\",\n",
       " \"('day', 'without')\",\n",
       " \"('live', 'sarcasm')\",\n",
       " \"('joke', 'sarcasm')\",\n",
       " \"('god', 'sarcasm')\",\n",
       " \"('player', 'sarcasm')\",\n",
       " \"('anymore', 'sarcasm')\",\n",
       " \"('sarcasm', 'smh')\",\n",
       " \"('looking', 'like')\",\n",
       " \"('ironic', 'life')\",\n",
       " \"('let', 'make')\",\n",
       " \"('oh', 'yay')\",\n",
       " \"('office', 'sarcasm')\",\n",
       " \"('thing', 'u')\",\n",
       " \"('weird', 'sarcasm')\",\n",
       " \"('new', 'orleans')\",\n",
       " \"('proud', 'sarcasm')\",\n",
       " \"('get', 'paid')\",\n",
       " \"('take', 'away')\",\n",
       " \"('winning', 'sarcasm')\",\n",
       " \"('radio', 'irony')\",\n",
       " \"('day', 'like')\",\n",
       " \"('break', 'irony')\",\n",
       " \"('irony', '😂😂😂')\",\n",
       " \"('passive', 'aggressive')\",\n",
       " \"('biggest', 'irony')\",\n",
       " \"('cant', 'stand')\",\n",
       " \"('tweet', 'ironic')\",\n",
       " \"('perfect', 'sarcasm')\",\n",
       " \"('two', 'day')\",\n",
       " \"('list', 'sarcasm')\",\n",
       " \"('video', 'sarcasm')\",\n",
       " \"('there', 'irony')\",\n",
       " \"('comment', 'sarcasm')\",\n",
       " \"('truck', 'irony')\",\n",
       " \"('anyone', 'ever')\",\n",
       " \"('wait', 'hear')\",\n",
       " \"('video', 'irony')\",\n",
       " \"('love', 'going')\",\n",
       " \"('appreciated', 'sarcasm')\",\n",
       " \"('sarcasm', 'best')\",\n",
       " \"('thing', 'get')\",\n",
       " \"('today', 'gon')\",\n",
       " \"('sore', 'throat')\",\n",
       " \"('really', 'mean')\",\n",
       " \"('paper', 'irony')\",\n",
       " \"('time', 'year')\",\n",
       " \"('people', 'really')\",\n",
       " \"('like', 'need')\",\n",
       " \"('ice', 'cream')\",\n",
       " \"('else', 'think')\",\n",
       " \"('pm', 'irony')\",\n",
       " \"('obama', 'sarcasm')\",\n",
       " \"('birthday', 'irony')\",\n",
       " \"('blog', 'post')\",\n",
       " \"('get', 'old')\",\n",
       " \"('said', 'want')\",\n",
       " \"('hope', 'amp')\",\n",
       " \"('thats', 'ironic')\",\n",
       " \"('going', 'make')\",\n",
       " \"('night', 'ironic')\",\n",
       " \"('fighting', 'irony')\",\n",
       " \"('always', 'make')\",\n",
       " \"('need', 'new')\",\n",
       " \"('one', 'see')\",\n",
       " \"('phone', 'call')\",\n",
       " \"('look', 'irony')\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bi = re2.nlargest(1000, 'Count')['Bigram'].tolist()\n",
    "top_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4185d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = ' '.join([word for word in text.split() if word not in top_bi])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae4f53",
   "metadata": {},
   "source": [
    "# N = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f52333de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Count  Total Count\n",
      "(got, ta, love)                                 29       145582\n",
      "(im, gon, na)                                   20       145582\n",
      "(much, fun, sarcasm)                            20       145582\n",
      "(cant, wait, see)                               16       145582\n",
      "(good, job, sarcasm)                            16       145582\n",
      "...                                            ...          ...\n",
      "(lifeimitatesart, irony, albertafreestore…)      1       145582\n",
      "(store, lifeimitatesart, irony)                  1       145582\n",
      "(free, store, lifeimitatesart)                   1       145582\n",
      "(slipper, free, store)                           1       145582\n",
      "(sarcasm, rwc, 🏉)                                1       145582\n",
      "\n",
      "[142805 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Select a particular class\n",
    "target_class = 'figurative'\n",
    "\n",
    "# Filter the dataset for the target class\n",
    "target_data = data[data['class'] == target_class]\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokenized_tweets = []\n",
    "for tweet in target_data['tweets']:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweets.append(tweet.split())\n",
    "\n",
    "# Generate trigrams\n",
    "tweet_trigrams = [list(trigrams(tweet_tokens)) for tweet_tokens in tokenized_tweets]\n",
    "\n",
    "# Flatten the list of trigrams\n",
    "all_trigrams = [trigram for tweet_trigram in tweet_trigrams for trigram in tweet_trigram]\n",
    "\n",
    "# Count the occurrences of each trigram\n",
    "trigram_counts = Counter(all_trigrams)\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "trigram_df = pd.DataFrame.from_dict(trigram_counts, orient='index', columns=['Count'])\n",
    "\n",
    "# Sort the DataFrame by the count column in descending order\n",
    "trigram_df = trigram_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Calculate the total count of trigrams in the target class\n",
    "total_count = trigram_df['Count'].sum()\n",
    "\n",
    "# Add a column with the total count for all classes\n",
    "trigram_df['Total Count'] = total_count\n",
    "\n",
    "print(trigram_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ec20627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(got, ta, love)</th>\n",
       "      <td>29</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(im, gon, na)</th>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(much, fun, sarcasm)</th>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cant, wait, see)</th>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(good, job, sarcasm)</th>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(lifeimitatesart, irony, albertafreestore…)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(store, lifeimitatesart, irony)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(free, store, lifeimitatesart)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(slipper, free, store)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sarcasm, rwc, 🏉)</th>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142805 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Count  Total Count\n",
       "(got, ta, love)                                 29       145582\n",
       "(im, gon, na)                                   20       145582\n",
       "(much, fun, sarcasm)                            20       145582\n",
       "(cant, wait, see)                               16       145582\n",
       "(good, job, sarcasm)                            16       145582\n",
       "...                                            ...          ...\n",
       "(lifeimitatesart, irony, albertafreestore…)      1       145582\n",
       "(store, lifeimitatesart, irony)                  1       145582\n",
       "(free, store, lifeimitatesart)                   1       145582\n",
       "(slipper, free, store)                           1       145582\n",
       "(sarcasm, rwc, 🏉)                                1       145582\n",
       "\n",
       "[142805 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61208780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "trigram_df.to_excel('result3.xlsx', index_label='trigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80998094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>Count</th>\n",
       "      <th>Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('got', 'ta', 'love')</td>\n",
       "      <td>29</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('im', 'gon', 'na')</td>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('much', 'fun', 'sarcasm')</td>\n",
       "      <td>20</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('cant', 'wait', 'see')</td>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('good', 'job', 'sarcasm')</td>\n",
       "      <td>16</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142800</th>\n",
       "      <td>('lifeimitatesart', 'irony', 'albertafreestore…')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142801</th>\n",
       "      <td>('store', 'lifeimitatesart', 'irony')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142802</th>\n",
       "      <td>('free', 'store', 'lifeimitatesart')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142803</th>\n",
       "      <td>('slipper', 'free', 'store')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142804</th>\n",
       "      <td>('sarcasm', 'rwc', '🏉')</td>\n",
       "      <td>1</td>\n",
       "      <td>145582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  trigram  Count  Total Count\n",
       "0                                   ('got', 'ta', 'love')     29       145582\n",
       "1                                     ('im', 'gon', 'na')     20       145582\n",
       "2                              ('much', 'fun', 'sarcasm')     20       145582\n",
       "3                                 ('cant', 'wait', 'see')     16       145582\n",
       "4                              ('good', 'job', 'sarcasm')     16       145582\n",
       "...                                                   ...    ...          ...\n",
       "142800  ('lifeimitatesart', 'irony', 'albertafreestore…')      1       145582\n",
       "142801              ('store', 'lifeimitatesart', 'irony')      1       145582\n",
       "142802               ('free', 'store', 'lifeimitatesart')      1       145582\n",
       "142803                       ('slipper', 'free', 'store')      1       145582\n",
       "142804                            ('sarcasm', 'rwc', '🏉')      1       145582\n",
       "\n",
       "[142805 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re3=pd.read_excel('result3.xlsx')\n",
    "re3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6842d022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('got', 'ta', 'love')\",\n",
       " \"('im', 'gon', 'na')\",\n",
       " \"('much', 'fun', 'sarcasm')\",\n",
       " \"('cant', 'wait', 'see')\",\n",
       " \"('good', 'job', 'sarcasm')\",\n",
       " \"('social', 'medium', 'irony')\",\n",
       " \"('keep', 'getting', 'better')\",\n",
       " \"('lowest', 'form', 'wit')\",\n",
       " \"('ha', 'ha', 'ha')\",\n",
       " \"('great', 'day', 'sarcasm')\",\n",
       " \"('see', 'coming', 'sarcasm')\",\n",
       " \"('well', 'done', 'sarcasm')\",\n",
       " \"('irony', 'fun', 'landlord')\",\n",
       " \"('ultimate', 'irony', 'fun')\",\n",
       " \"('landlord', 'ultimate', 'irony')\",\n",
       " \"('fun', 'landlord', 'nosmoking')\",\n",
       " \"('nosmoking', 'smoking', 'irony')\",\n",
       " \"('landlord', 'nosmoking', 'smoking')\",\n",
       " \"('make', 'sense', 'sarcasm')\",\n",
       " \"('always', 'fun', 'sarcasm')\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tri = re3.nlargest(20, 'Count')['trigram'].tolist()\n",
    "top_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df05ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = ' '.join([word for word in text.split() if word not in top_tri])\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'tweets' column\n",
    "df['tweets'] = df['tweets'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74a809f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail reader sensible always shocker sarca...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get feeling like game sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably missed text sarcastic</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81403</th>\n",
       "      <td>photo image via heart childhood cool funny sar...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81404</th>\n",
       "      <td>never knewi better put universe lolmaybe there...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81405</th>\n",
       "      <td>hey wanted say thanks puberty letting apart it...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81406</th>\n",
       "      <td>im sure coverage like fox news special “ hidde...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81407</th>\n",
       "      <td>u believe see p sarcasm</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets       class\n",
       "0      aware dirty step get money staylight staywhite...  figurative\n",
       "1                sarcasm people understand diy artattack  figurative\n",
       "2      dailymail reader sensible always shocker sarca...  figurative\n",
       "3                          get feeling like game sarcasm  figurative\n",
       "4                         probably missed text sarcastic  figurative\n",
       "...                                                  ...         ...\n",
       "81403  photo image via heart childhood cool funny sar...     sarcasm\n",
       "81404  never knewi better put universe lolmaybe there...     sarcasm\n",
       "81405  hey wanted say thanks puberty letting apart it...     sarcasm\n",
       "81406  im sure coverage like fox news special “ hidde...     sarcasm\n",
       "81407                            u believe see p sarcasm     sarcasm\n",
       "\n",
       "[81408 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01f4152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7224542439503746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarth Naik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Tokenized tweets\n",
    "tokenized_tweets = [tweet.split() for tweet in df['tweets']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert tweets to average word embeddings\n",
    "tweet_embeddings = []\n",
    "for tweet in tokenized_tweets:\n",
    "    embeddings = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if embeddings:\n",
    "        tweet_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        tweet_embedding = np.zeros(100)  # Use zero vector if no word embeddings found\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Prepare data for training\n",
    "X = np.vstack(tweet_embeddings)\n",
    "y = df['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ccb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10434390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarth Naik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  figurative       0.11      0.09      0.10      4179\n",
      "       irony       0.43      0.55      0.48      4276\n",
      "     regular       0.93      0.74      0.82      3696\n",
      "     sarcasm       0.54      0.55      0.54      4131\n",
      "\n",
      "    accuracy                           0.47     16282\n",
      "   macro avg       0.50      0.48      0.49     16282\n",
      "weighted avg       0.49      0.47      0.48     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the tweet data and labels\n",
    "\n",
    "tweets = df['tweets']\n",
    "labels = df['class']\n",
    "\n",
    "# Preprocess the tweets (e.g., lowercasing, removing punctuation, etc.)\n",
    "# ...\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the vectorizer with bigram features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Transform the training data into bigram feature vectors\n",
    "train_features = vectorizer.fit_transform(train_tweets)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Build a classification model (e.g., logistic regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2247c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdf9f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(lr_model, open('clsf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd52eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
